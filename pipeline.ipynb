{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metabarcoding Pipeline\n",
    "======================\n",
    "\n",
    "Frédéric Mahé, April 19th 2023\n",
    "\n",
    "A fast and accurate metabarcoding pipeline.\n",
    "\n",
    "Contrary to traditional pipelines, most filtering steps are done after\n",
    "the clustering/denoising step, when the risk of eliminating real\n",
    "molecular diversity is minimized.\n",
    "\n",
    "Please feel free to ask questions at anytime during this hands-on\n",
    "session.\n",
    "\n",
    "Part 0: crash-course and set-up\n",
    "-------------------------------\n",
    "\n",
    "A quick introduction on the type of code you will see today.\n",
    "\n",
    "### shell\n",
    "\n",
    "#### code block\n",
    "\n",
    "My pipeline is made of blocks of shell code:\n",
    "\n",
    "``` shell\n",
    "# variables\n",
    "THREADS=4\n",
    "ENCODING=33\n",
    "\n",
    "# some comments\n",
    "vsearch \\\n",
    "    --threads ${THREADS} \\\n",
    "    --fastq_mergepairs R1.fastq.gz \\\n",
    "    --reverse R2.fastq.gz \\\n",
    "    --fastq_ascii ${ENCODING} \\\n",
    "    --fastq_allowmergestagger \\\n",
    "    --quiet \\\n",
    "    --fastqout out.fastq\n",
    "```\n",
    "\n",
    "Some blocks can be executed in Google colab. Blocks that can’t be\n",
    "executed are examples, like the one above. They are not necessary for\n",
    "the pipeline.\n",
    "\n",
    "#### redirect\n",
    "\n",
    "You can redirect the output of a shell command:\n",
    "\n",
    "``` shell\n",
    "# basics\n",
    "command > output.fastq\n",
    "command 2> output.log\n",
    "command 2> /dev/null\n",
    "command < input.fastq\n",
    "\n",
    "# but also\n",
    ">>  2>>  2>&1  <(...)\n",
    "```\n",
    "\n",
    "Note: I’ve been writing shell scripts daily for nearly 20 years, and I\n",
    "may use lesser known aspects of bash (and not know about recent\n",
    "features). Feel free to ask or comment if my code is unclear.\n",
    "\n",
    "#### wrap\n",
    "\n",
    "A matter of personal preference:\n",
    "\n",
    "``` shell\n",
    "# too long to read:\n",
    "vsearch --threads 4 --fastq_mergepairs R1.fastq.gz --reverse R2.fastq.gz --fastq_ascii 33 --fastq_allowmergestagger --quiet --fastqout out.fastq\n",
    "\n",
    "# wrapping makes it more readable:\n",
    "vsearch \\\n",
    "    --threads 4 \\\n",
    "    --fastq_mergepairs R1.fastq.gz \\\n",
    "    --reverse R2.fastq.gz \\\n",
    "    --fastq_ascii 33 \\\n",
    "    --fastq_allowmergestagger \\\n",
    "    --quiet \\\n",
    "    --fastqout out.fastq\n",
    "```\n",
    "\n",
    "#### pipe\n",
    "\n",
    "make data flow! A remarkable asset of the Unix/Linux world:\n",
    "\n",
    "``` shell\n",
    "# slow\n",
    "command1 input.fastq > tmp1.fastq\n",
    "command2 tmp1.fastq > tmp2.fastq\n",
    "command3 tmp2.fastq > final_output.fastq\n",
    "\n",
    "# use pipes to avoid temporary files:\n",
    "command1 input.fastq | \\\n",
    "    command2 | \\\n",
    "    command3 > final_output.fastq\n",
    "```\n",
    "\n",
    "Note: pipes were recently added to R, C++, and other languages.\n",
    "\n",
    "#### tee\n",
    "\n",
    "Use a `tee` to save an intermediary result:\n",
    "\n",
    "``` shell\n",
    "command1 input.fastq | \\\n",
    "    command2 | \\\n",
    "    tee output2.fastq | \\\n",
    "    command3 > final_output.fastq\n",
    "```\n",
    "\n",
    "A `tee` will duplicate your stream of data, allowing you to do process\n",
    "it in two different ways simultaneously. It is possible to connect\n",
    "multiple `tee`s and to create a multi-furcation.\n",
    "\n",
    "#### test\n",
    "\n",
    "The shell is a great tool to manipulate text. You can easily create fake\n",
    "data and pass it to a software you would like to test:\n",
    "\n",
    "``` shell\n",
    "# create toy-examples:\n",
    "printf \">s_1\\nA\\n\"\n",
    "\n",
    "# use them to test software behavior:\n",
    "printf \">s_1\\nA\\n\" | \\\n",
    "    swarm\n",
    "```\n",
    "\n",
    "Note: documentation rarely is 100% complete, when you have a doubt about\n",
    "a tool, create a toy-example to test its behavior. We’ll dive deeper\n",
    "into code testing this afternoon during the `lulu` session.\n",
    "\n",
    "### FASTQ format\n",
    "\n",
    "Metabarcoding data are usually available in\n",
    "[FASTQ](https://en.wikipedia.org/wiki/FASTQ_format#Encoding) format:\n",
    "\n",
    "-   most-frequent format,\n",
    "-   human-readable,\n",
    "-   encode quality values (probability of error for each position),\n",
    "-   can be hard to parse,\n",
    "-   encoding type must be guessed\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "    @M05074:97:000000000-BPW9G:1:1101:10203:1383 1:N:0:2\n",
    "    CATAATTTCCTCCGCTTATTGATATGCTTAAGTTCAGCGGGTATCCCTACCTGATCCGAGTTCAACCTAAGAAAGTTGGGGGTTCTGGCGGGTGGACGGCTGAACCCTGTAGCGACAAGTATTACTACGCTTAGAGCCAGACGGCACCGCCACTGCTTTTAAGTGCCGCCGGTACAGCGGGCCCCAAGGCCAAGCAGAGCTTGATTGGTCA\n",
    "    +\n",
    "    @-A-9EFGFFFFD7BFF7FE9,C9F<EFG99,CEF9,@77+@+CCC@F9FCF9,C@C,,+,8C9<CEF,,,,,,,CF,,+++8FEF9,?+++@+++B++@C+,,B?FE8E,,<+++++3C,CF9DF9>>CFE7,,3=@7,,@++@:FC7BC*CC:,7>DF9,,,,7?*=B*5?*:++7***=?EE3***2;***:*0*/;@C8*<C+*<<+\n",
    "\n",
    "Note: the quality line may starts with a @ …\n",
    "\n",
    "Note: Q values are a way to encode on one character numerical values\n",
    "ranging from 0 to 41 (usually). These values represent the probability\n",
    "of a wrong base calling for that particular position. Q20 means 1% of\n",
    "risk, Q30 means 0.1%, and Q40 means 0.01%.\n",
    "\n",
    "In paired-end sequencing, there are two files per sample: R1 and R2.\n",
    "Each R1 read has a R2 counterpart, and reads are in the same order in\n",
    "both files.\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "Let’s explore this environment. This is our first executable block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "date\n",
    "whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are `root`! Maximal clearance level, we can do anything we want.\n",
    "\n",
    "Check the OS version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "lsb_release -a\n",
    "uname -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operating system is Ubuntu LTS 20.04. LTS stands for long-term\n",
    "support, the most recent LTS at the time of writing is 22.04. Version\n",
    "20.04 will be supported until April 2025.\n",
    "\n",
    "Check basic utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "bash --version\n",
    "git --version\n",
    "gcc --version\n",
    "python --version\n",
    "R --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`R`, `git` and the compilation tool-chain are already installed.\n",
    "\n",
    "The `gcc` version is a bit old (9.2), and might not allow to compile\n",
    "code based on very recent standards (e.g.; `mumu` which uses C++20\n",
    "features).\n",
    "\n",
    "What about hardware resources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "df -h\n",
    "cat /proc/cpuinfo\n",
    "cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Google colab instances are virtual x86-64 machines with 2\n",
    "CPU-cores (Intel Xeon @ 2.20GHz), 12 GB of RAM and 85 GB of storage\n",
    "space.\n",
    "\n",
    "I will assume that you also have [python](https://www.python.org/)\n",
    "(version 3.7 or more), [R](https://cran.r-project.org/) (version 4.0 or\n",
    "more), and [bash](https://www.gnu.org/software/bash/) (version 4 or\n",
    "more).\n",
    "\n",
    "Last check : is it possible to pass data from one `shell` code block to\n",
    "another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "pwd\n",
    "mkdir -p tmp\n",
    "cd ./tmp/\n",
    "pwd\n",
    "i=5\n",
    "export j=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "pwd\n",
    "echo \"i=\"$i\n",
    "echo \"j=\"$j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that in `%%shell` code blocks, `cd` moves, variable\n",
    "declarations, and function declarations are limited to the current block\n",
    "(no effect on downstream code blocks). Only file and folder creations\n",
    "are persistent.\n",
    "\n",
    "### install dependencies\n",
    "\n",
    "Let’s create some folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "mkdir -p src data references results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to install [vsearch](https://github.com/torognes/vsearch),\n",
    "[cutadapt](https://github.com/marcelm/cutadapt/), and\n",
    "[swarm](https://github.com/torognes/swarm).\n",
    "\n",
    "Installing [lulu](https://github.com/tobiasgf/lulu) or\n",
    "[mumu](https://github.com/frederic-mahe/mumu) is not necessary in this\n",
    "particular pipeline.\n",
    "\n",
    "#### install cutadapt\n",
    "\n",
    "We could install the Ubuntu version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "apt update\n",
    "apt search cutadapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it is a bit old. Let’s install `cutadapt` with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install --upgrade cutadapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cutadapt --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are using the apt system, let’s install additional tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "apt install dos2unix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(not important, but useful to process the reference database)\n",
    "\n",
    "#### install swarm\n",
    "\n",
    "We could install `swarm` and `vsearch` using `conda`, but for\n",
    "educational purposes, let’s compile them ourselves. We will put their\n",
    "source code in the `/tmp` folder (this folder is cleaned automatically\n",
    "between reboots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd /tmp/\n",
    "git clone https://github.com/torognes/swarm.git\n",
    "cd ./swarm/\n",
    "make --jobs\n",
    "make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "swarm --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "rm --recursive /tmp/swarm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install vsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd /tmp/\n",
    "git clone https://github.com/torognes/vsearch.git\n",
    "cd ./vsearch/\n",
    "./autogen.sh\n",
    "./configure CFLAGS=\"-O3\" CXXFLAGS=\"-O3\"\n",
    "make --jobs\n",
    "make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "vsearch --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "rm --recursive /tmp/vsearch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install python scripts\n",
    "\n",
    "In the second part of the pipeline, we are going to use four python\n",
    "scripts to build and update occurrence tables, and to compute\n",
    "last-common ancestor taxonomic assignments.\n",
    "\n",
    "The scripts are available on GitHub, let’s download the latest versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./src/\n",
    "\n",
    "# occurrence table creation\n",
    "URL=\"https://raw.githubusercontent.com/frederic-mahe/fred-metabarcoding-pipeline/master/src\"\n",
    "for SCRIPT in \"OTU_cleaver\" \"OTU_contingency_table_filtered\" \"OTU_table_updater\" ; do\n",
    "    wget --continue \"${URL}/${SCRIPT}.py\"\n",
    "done\n",
    "\n",
    "# taxonomic assignment\n",
    "URL=\"https://raw.githubusercontent.com/frederic-mahe/stampa/master\"\n",
    "wget --continue \"${URL}/stampa_merge.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequencing data\n",
    "\n",
    "Today, we are going to use a subset of the Neotropical Forest Soil\n",
    "dataset\n",
    "([PRJNA317860](https://www.ebi.ac.uk/ena/browser/view/PRJNA317860);\n",
    "[Mahé et al., 2017](https://doi.org/10.1038/s41559-017-0091)),\n",
    "corresponding to the following ENA/SRA run accessions:\n",
    "\n",
    "| runs        | sample |\n",
    "|-------------|--------|\n",
    "| SRR23272700 | B070   |\n",
    "| SRR23272716 | B030   |\n",
    "| SRR23272737 | B100   |\n",
    "| SRR23272741 | B060   |\n",
    "| SRR23272752 | B050   |\n",
    "| SRR23272767 | L040   |\n",
    "| SRR23272778 | L030   |\n",
    "| SRR23272788 | B090   |\n",
    "| SRR23272799 | B080   |\n",
    "| SRR23272803 | B040   |\n",
    "| SRR23272822 | L080   |\n",
    "| SRR23272833 | L070   |\n",
    "| SRR23272848 | L020   |\n",
    "| SRR23272859 | L010   |\n",
    "| SRR23272860 | B020   |\n",
    "| SRR23272861 | B010   |\n",
    "| SRR23272874 | L090   |\n",
    "| SRR23272881 | L100   |\n",
    "| SRR23272890 | L060   |\n",
    "| SRR23272901 | L050   |\n",
    "\n",
    "and subsampled at 1%, using `vsearch`:\n",
    "\n",
    "``` shell\n",
    "function subsample() {\n",
    "    local -ri SEED=1\n",
    "    local -r PERCENTAGE=\"1.0\"\n",
    "    local -r SUBSAMPLED_FASTQ=\"$(sed 's/NG-7070_// ; s/_lib.*_1976//' <<< ${1})\"\n",
    "\n",
    "    vsearch \\\n",
    "        --fastx_subsample \"${1}\" \\\n",
    "        --randseed \"${SEED}\" \\\n",
    "        --sample_pct \"${PERCENTAGE}\" \\\n",
    "        --quiet \\\n",
    "        --fastqout - | \\\n",
    "        gzip - > \"${SUBSAMPLED_FASTQ}\"\n",
    "}\n",
    "\n",
    "\n",
    "export -f subsample\n",
    "\n",
    "find . -name \"NG-7070_*.fastq.gz\" -type f -exec bash -c 'subsample \"$0\"' {} \\;\n",
    "```\n",
    "\n",
    "Subsampling is useful when you are developing and testing a new\n",
    "pipeline. The dataset is smaller but remains realistic, allowing for a\n",
    "faster development cycle. With `vsearch`, you can also subsample down to\n",
    "a particular number of reads.\n",
    "\n",
    "Combining `vsearch` with the command `find` offers a very powerful and\n",
    "robust way to find and subsample all fastq files (it will find every\n",
    "file, including files in sub-folders). You might find a loop-based\n",
    "approach easier to read:\n",
    "\n",
    "``` shell\n",
    "for FASTQ in \"NG-7070_*.fastq.gz\" ; do\n",
    "    subsample \"${FASTQ}\"\n",
    "done\n",
    "```\n",
    "\n",
    "Note: in a pair of R1 and R2 fastq files, both files have the same\n",
    "number of reads, so using a fix seed (i.e., not zero) guarantees that\n",
    "subsamplings results for both R1 and R2 fastq files are identical (same\n",
    "number of reads, same reads, in the same order)\n",
    "\n",
    "These 20 runs represent 4 GB of compressed data. To save time and\n",
    "energy, we are going to download the subsampled files directly (roughly\n",
    "40 MB in total). The files are hosted on my GitHub account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "\n",
    "URL=\"https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data\"\n",
    "\n",
    "wget \"${URL}/MD5SUM\"\n",
    "for SAMPLE in B010 B020 B030 B040 B050 B060 B070 B080 B090 B100 L010 L020 L030 L040 L050 L060 L070 L080 L090 L100 ; do\n",
    "    for READ in 1 2 ; do\n",
    "        wget --continue \"${URL}/${SAMPLE}_1_${READ}.fastq.gz\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "md5sum -c MD5SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial situation: fastq files are already demultiplexed, we have a pair\n",
    "of R1 and R2 files for each sample.\n",
    "\n",
    "Part 1: from fastq files to fasta files\n",
    "---------------------------------------\n",
    "\n",
    "![pipeline\n",
    "overview](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/diapo_pipeline_final_colour.png)\n",
    "\n",
    "The pipeline is divided into two parts. A first part where each sample\n",
    "is processed individually. And a second part where all samples are\n",
    "pooled to produce an occurrence table.\n",
    "\n",
    "In this first part of the pipeline, we will: 1. merge R1 and R2, 1. trim\n",
    "primers, 1. convert fastq to fasta, 1. extract expected errors (for a\n",
    "later *quality-based filtering*), 1. dereplicate fasta, 1. per-sample\n",
    "clustering (for a later *cluster cleaving*)\n",
    "\n",
    "The code below uses named pipes (`fifo`) to avoid writing intermediate\n",
    "results to mass storage. The goal is to speed up processing, and to make\n",
    "the code more modular and clearer. On the other hand, `fifo`s are tricky\n",
    "to use, as you must remember to launch producers and consumers in the\n",
    "background before running the last consumer.\n",
    "\n",
    "Don’t panic! While the script is running, we are going to look at each\n",
    "function and explain what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "export LC_ALL=C\n",
    "\n",
    "## ------------------------------------------------------------ define variables\n",
    "declare -r PRIMER_F=\"CCAGCASCYGCGGTAATTCC\"\n",
    "declare -r PRIMER_R=\"ACTTTCGTTCTTGATYRA\"\n",
    "declare -r FASTQ_NAME_PATTERN=\"*_1.fastq.gz\"\n",
    "declare -ri THREADS=2\n",
    "declare -r CUTADAPT_OPTIONS=\"--minimum-length 32 --cores=${THREADS} --discard-untrimmed\"\n",
    "declare -r CUTADAPT=\"$(which cutadapt) ${CUTADAPT_OPTIONS}\"  # cutadapt 4.1 or more recent\n",
    "declare -r SWARM=\"$(which swarm)\"  # swarm 3.0 or more recent\n",
    "declare -r VSEARCH=\"$(which vsearch) --quiet\"  # vsearch 2.21.1 or more recent\n",
    "declare -ri ENCODING=33\n",
    "declare -r MIN_F=$(( ${#PRIMER_F} * 2 / 3 ))  # match is >= 2/3 of primer length\n",
    "declare -r MIN_R=$(( ${#PRIMER_R} * 2 / 3 ))\n",
    "declare -r FIFOS=$(echo fifo_{merged,trimmed}_fastq fifo_filtered_fasta{,_bis})\n",
    "declare -i TICKER=0\n",
    "\n",
    "## ------------------------------------------------------------------- functions\n",
    "revcomp() {\n",
    "    # reverse-complement a DNA/RNA IUPAC string\n",
    "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
    "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
    "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
    "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
    "}\n",
    "\n",
    "merge_fastq_pair() {\n",
    "    ${VSEARCH} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --fastq_mergepairs \"${FORWARD}\" \\\n",
    "        --reverse \"${REVERSE}\" \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --fastq_allowmergestagger \\\n",
    "        --fastqout fifo_merged_fastq 2> \"${SAMPLE}.log\" &\n",
    "}\n",
    "\n",
    "trim_primers() {\n",
    "    # search forward primer in both normal and revcomp: now all reads\n",
    "    # are in the same orientation\n",
    "    ${CUTADAPT} \\\n",
    "        --revcomp \\\n",
    "        --front \"${PRIMER_F};rightmost\" \\\n",
    "        --overlap \"${MIN_F}\" fifo_merged_fastq 2>> \"${SAMPLE}.log\" | \\\n",
    "        ${CUTADAPT} \\\n",
    "            --adapter \"${ANTI_PRIMER_R}\" \\\n",
    "            --overlap \"${MIN_R}\" \\\n",
    "            --max-n 0 - > fifo_trimmed_fastq 2>> \"${SAMPLE}.log\" &\n",
    "}\n",
    "\n",
    "convert_fastq_to_fasta() {\n",
    "    # use SHA1 values as sequence names,\n",
    "    # compute expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --fastq_filter fifo_trimmed_fastq \\\n",
    "        --relabel_sha1 \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --eeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --fastaout - 2>> \"${SAMPLE}.log\" | \\\n",
    "        tee fifo_filtered_fasta_bis > fifo_filtered_fasta &\n",
    "}\n",
    "\n",
    "extract_expected_error_values() {\n",
    "    # extract ee for future quality filtering (keep the lowest\n",
    "    # observed expected error value for each unique sequence)\n",
    "    local -ri length_of_sequence_IDs=40\n",
    "    paste - - < fifo_filtered_fasta_bis | \\\n",
    "        awk 'BEGIN {FS = \"[>;=\\t]\"} {print $2, $4, length($NF)}' | \\\n",
    "        sort --key=3,3n --key=1,1d --key=2,2n | \\\n",
    "        uniq --check-chars=${length_of_sequence_IDs} > \"${SAMPLE}.qual\" &\n",
    "}\n",
    "\n",
    "dereplicate_fasta() {\n",
    "    # dereplicate and discard expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --derep_fulllength fifo_filtered_fasta \\\n",
    "        --sizeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --xee \\\n",
    "        --output \"${SAMPLE}.fas\" 2>> \"${SAMPLE}.log\"\n",
    "}\n",
    "\n",
    "list_local_clusters() {\n",
    "    # retain only clusters with more than 2 reads\n",
    "    # (do not use the fastidious option here)\n",
    "    ${SWARM} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --differences 1 \\\n",
    "        --usearch-abundance \\\n",
    "        --log /dev/null \\\n",
    "        --output-file /dev/null \\\n",
    "        --statistics-file - \\\n",
    "        \"${SAMPLE}.fas\" | \\\n",
    "        awk 'BEGIN {FS = OFS = \"\\t\"} $2 > 2' > \"${SAMPLE}.stats\"\n",
    "}\n",
    "\n",
    "## ------------------------------------------------------------------------ main\n",
    "# from raw fastq files to ready-to-use sample files\n",
    "declare -r ANTI_PRIMER_R=\"$(revcomp \"${PRIMER_R}\")\"\n",
    "find . -name \"${FASTQ_NAME_PATTERN}\" -type f -print0 | \\\n",
    "    while IFS= read -r -d '' FORWARD ; do\n",
    "        TICKER=$(( $TICKER + 1 ))\n",
    "        echo -e \"${TICKER}\\t${FORWARD}\"\n",
    "        REVERSE=\"${FORWARD/_1\\./_2.}\"  # adapt to fastq name patterns\n",
    "        SAMPLE=\"${FORWARD/_1_1.*/}\"\n",
    "\n",
    "        # clean (remove older files, if any)\n",
    "        rm --force \"${SAMPLE}\".{fas,qual,log,stats} ${FIFOS}\n",
    "        mkfifo ${FIFOS}\n",
    "\n",
    "        merge_fastq_pair\n",
    "        trim_primers\n",
    "        convert_fastq_to_fasta\n",
    "        extract_expected_error_values\n",
    "        dereplicate_fasta\n",
    "        list_local_clusters\n",
    "\n",
    "        # clean (make sure fifos are not reused)\n",
    "        rm ${FIFOS}\n",
    "        unset FORWARD REVERSE SAMPLE\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adapt this code to another dataset, you just need to change the\n",
    "primer sequences in the initial block of variables, and the raw fastq\n",
    "file search pattern and sample file naming (in the final `while` loop),\n",
    "if your raw fastq files follow another naming rule.\n",
    "\n",
    "Let’s have a look at the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable declarations\n",
    "\n",
    "``` shell\n",
    "VAR1=\"some text\"\n",
    "declare -r VAR1=\"more text\"\n",
    "declare -ri VAR2=42\n",
    "```\n",
    "\n",
    "Here, we are using `declare -r` to indicate that `VAR1` is a constant.\n",
    "If we try to modify it somewhere in the code, that’s a bug, and the\n",
    "execution will stop with an error message.\n",
    "\n",
    "`declare -ri` indicates that `VAR2` is a constant integer.\n",
    "\n",
    "### R1 and R2 merging\n",
    "\n",
    "``` shell\n",
    "merge_fastq_pair() {\n",
    "    ${VSEARCH} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --fastq_mergepairs \"${FORWARD}\" \\\n",
    "        --reverse \"${REVERSE}\" \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --fastq_allowmergestagger \\\n",
    "        --fastqout fifo_merged_fastq 2> \"${SAMPLE}.log\" &\n",
    "}\n",
    "```\n",
    "\n",
    "before merging:\n",
    "\n",
    "    R1: ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequen\n",
    "                                                   ||||\n",
    "                                                   quence-ESREVER_REMIRP-GAT-ROTPADA :R2\n",
    "\n",
    "after merging:\n",
    "\n",
    "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "A minimal overlap and similarity is required for the merging. Reads that\n",
    "can’t be merged are discarded. `vsearch` re-computes the quality values\n",
    "of the overlapping positions. An overlap corresponds to a double\n",
    "observation, and the error probability must then be re-evaluated for\n",
    "each position in the overlap. Some mergers do it the wrong way.\n",
    "\n",
    "![fastq\\_merging\\_theory](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/fastq_merging_theory.png)\n",
    "\n",
    "Note: according to [Edgar and Flyvberg\n",
    "(2015)](https://doi.org/10.1093/bioinformatics/btv401) merging is an\n",
    "important step that can change radically the apparent diversity profile\n",
    "of a community (some popular mergers do not cope well with variable\n",
    "length markers).\n",
    "\n",
    "### reverse-complement\n",
    "\n",
    "``` shell\n",
    "revcomp() {\n",
    "    # reverse-complement a DNA/RNA IUPAC string\n",
    "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
    "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
    "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
    "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
    "}\n",
    "```\n",
    "\n",
    "That function takes a primer sequence, for example `ACTTTCGTTCTTGATYRA`\n",
    "and outputs the reverse-complement of that sequence\n",
    "(`TYRATCAAGAACGAAAGT`). This is useful when searching for primers after\n",
    "merging, because the reverse primer is reverse-complemented in merged\n",
    "reads:\n",
    "\n",
    "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "### trim primers\n",
    "\n",
    "After merging, we use cutadapt to trim primers:\n",
    "\n",
    "``` shell\n",
    "trim_primers() {\n",
    "    # search forward primer in both normal and revcomp: now all reads\n",
    "    # are in the same orientation\n",
    "    ${CUTADAPT} \\\n",
    "        --revcomp \\\n",
    "        --front \"${PRIMER_F};rightmost\" \\\n",
    "        --overlap \"${MIN_F}\" fifo_merged_fastq 2>> \"${SAMPLE}.log\" | \\\n",
    "        ${CUTADAPT} \\\n",
    "            --adapter \"${ANTI_PRIMER_R}\" \\\n",
    "            --overlap \"${MIN_R}\" \\\n",
    "            --max-n 0 - > fifo_trimmed_fastq 2>> \"${SAMPLE}.log\" &\n",
    "}\n",
    "```\n",
    "\n",
    "Before:\n",
    "\n",
    "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "the first call removes the *rightmost* forward primer (and everything\n",
    "before):\n",
    "\n",
    "                               actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "the second call removes the reverse primer (and everything after).\n",
    "\n",
    "                               actual_target_sequence\n",
    "\n",
    "At either step, if a primer is not found, the read is discarded.\n",
    "\n",
    "`--revcomp`: some sequencing protocols can produce reads in random\n",
    "orientations (R1 contains a mix of forward and reverse reads). That\n",
    "option has the effect of re-orienting the reads.\n",
    "\n",
    "`--overlap`: by default, cutadapt considers that an overlap of three\n",
    "nucleotides is enough for a match. Here we require a minimal overlap\n",
    "equal to 2/3rd of the length of our primers.\n",
    "\n",
    "    Match:\n",
    "\n",
    "       MER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "       |||||||||||\n",
    "    PRIMER_FORWARD\n",
    "\n",
    "\n",
    "    No Match\n",
    "              WARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "              ||||\n",
    "    PRIMER_FORWARD\n",
    "\n",
    "Note: with `--max-n 0`, reads with uncertain nucleotides (`N`) are\n",
    "discarded.\n",
    "\n",
    "### convert fastq to fasta\n",
    "\n",
    "A simple format conversion:\n",
    "\n",
    "``` shell\n",
    "convert_fastq_to_fasta() {\n",
    "    # use SHA1 values as sequence names,\n",
    "    # compute expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --fastq_filter fifo_trimmed_fastq \\\n",
    "        --relabel_sha1 \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --eeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --fastaout - 2>> \"${SAMPLE}.log\" | \\\n",
    "        tee fifo_filtered_fasta_bis > fifo_filtered_fasta &\n",
    "}\n",
    "```\n",
    "\n",
    "For example, a fastq entry such as:\n",
    "\n",
    "    @s1\n",
    "    AAAA\n",
    "    +\n",
    "    IIII\n",
    "\n",
    "will be transformed into a `fasta` entry, with a sequence-derived name\n",
    "(hash value of the sequence), and the computed expected error (later\n",
    "used for quality filtering):\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
    "    AAAA\n",
    "\n",
    "Why using hash values as sequence names?\n",
    "\n",
    "-   easy to compute,\n",
    "-   compact fix length name,\n",
    "-   readable (you get use to it),\n",
    "-   a given sequence will always produce the same hash,\n",
    "-   allows cross-studies comparisons\n",
    "\n",
    "### extract expected error values\n",
    "\n",
    "Extract and store read lengths and expected error values (later used for\n",
    "quality filtering). If two reads have the exact same sequence, keep the\n",
    "best (lowest) expected error.\n",
    "\n",
    "``` shell\n",
    "extract_expected_error_values() {\n",
    "    # extract ee for future quality filtering (keep the lowest\n",
    "    # observed expected error value for each unique sequence)\n",
    "    local -ri length_of_sequence_IDs=40\n",
    "    paste - - < fifo_filtered_fasta_bis | \\\n",
    "        awk 'BEGIN {FS = \"[>;=\\t]\"} {print $2, $4, length($NF)}' | \\\n",
    "        sort --key=3,3n --key=1,1d --key=2,2n | \\\n",
    "        uniq --check-chars=${length_of_sequence_IDs} > \"${SAMPLE}.qual\" &\n",
    "}\n",
    "```\n",
    "\n",
    "The result is a file with three columns: 1. amplicon name, 1. lowest\n",
    "expected error observed for that amplicon in that sample, 1. amplicon\n",
    "length\n",
    "\n",
    "    17765e625e2e3588a21306012dfd8f162b8944b8 0.003813 48\n",
    "    3dd5e4500bd663e4dd27bf0e2ae5b41cb9eee2b9 0.004051 51\n",
    "    138408e37cf3c86e025cf6d5f6828eff9dbd1015 0.004448 56\n",
    "    75f075940a92869137394c5a0b9d06d5c4aad2f6 0.5106 57\n",
    "    1a8baf082848f50d2019ed162c22e87e02122868 0.004766 60\n",
    "    3b6f64c93290cee1f1102065ddcb345355e0f223 0.005084 64\n",
    "    d0b25e99d48fd996dedb179cefae6941c768abb5 0.005322 67\n",
    "    319f8eb5171101539d3fb7255c3edd500ec5eb6b 0.005481 69\n",
    "    28d992d5a8900d23be60dee0e07f27872a30f770 0.03710 70\n",
    "    c7c55f0c514ce8c2a5ee49f02523dadcd96a4109 0.01171 78\n",
    "    ...\n",
    "\n",
    "Note: that function could be simplified now that `vsearch` version 2.23\n",
    "can add length attributes `;length=123` to fastq and fasta headers.\n",
    "\n",
    "### dereplicate\n",
    "\n",
    "The first significant lossless reduction of our dataset! In\n",
    "metabarcoding datasets, some sequences are rare and only observed once,\n",
    "whereas some sequences are present in many exact copies. When coded\n",
    "correctly, finding identical sequences is a very fast and efficient\n",
    "operation:\n",
    "\n",
    "``` shell\n",
    "dereplicate_fasta() {\n",
    "    # dereplicate and discard expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --derep_fulllength fifo_filtered_fasta \\\n",
    "        --sizeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --xee \\\n",
    "        --output \"${SAMPLE}.fas\" 2>> \"${SAMPLE}.log\"\n",
    "}\n",
    "```\n",
    "\n",
    "For instance, this fasta file:\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
    "    AAAA\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
    "    AAAA\n",
    "\n",
    "will be dereplicated into this one:\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=2\n",
    "    AAAA\n",
    "\n",
    "Note that expected error values are removed by the `-xee` option, and\n",
    "that a new attribute `;size=2` has been added to represent the fact that\n",
    "this particular sequence has been observed twice.\n",
    "\n",
    "### list local clusters\n",
    "\n",
    "Later in this analysis, we will need to search for cluster\n",
    "co-occurrences on a per-sample basis. For each sample, we quickly\n",
    "generate clusters and a list of cluster seeds with `swarm` and store the\n",
    "results. Here we use `swarm` for the first time, I will give more\n",
    "details when `swarm` will be used to process the whole dataset (pooled\n",
    "samples), in the second part of the pipeline.\n",
    "\n",
    "``` shell\n",
    "list_local_clusters() {\n",
    "    # retain only clusters with more than 2 reads\n",
    "    # (do not use the fastidious option here)\n",
    "    ${SWARM} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --differences 1 \\\n",
    "        --usearch-abundance \\\n",
    "        --log /dev/null \\\n",
    "        --output-file /dev/null \\\n",
    "        --statistics-file - \\\n",
    "        \"${SAMPLE}.fas\" | \\\n",
    "        awk 'BEGIN {FS = OFS = \"\\t\"} $2 > 2' > \"${SAMPLE}.stats\"\n",
    "}\n",
    "```\n",
    "\n",
    "Here, `swarm` reads a fasta file, representing a sample, and produces a\n",
    "table looking like that:\n",
    "\n",
    "| uniq | abundance | seed                                     | abundance | singletons | layers | steps |\n",
    "|------|-----------|------------------------------------------|-----------|------------|--------|-------|\n",
    "| 5897 | 22010     | 3db68d2f77252793f23d7089d0d4103eb8942dcb | 3612      | 4600       | 8      | 8     |\n",
    "| 5528 | 15548     | 45211af6b7811bf45b5d3694054b800b5b13efd4 | 3728      | 4488       | 9      | 9     |\n",
    "| 3058 | 13452     | 47e639615ad19c8dededae45f38beb52c4e9861d | 3542      | 2252       | 9      | 9     |\n",
    "| 2172 | 8268      | 307ab3d7f513adc74854dd72e817fe02f7601db2 | 3722      | 1639       | 8      | 8     |\n",
    "| 1453 | 6754      | e52a67c43758a5f9a39b4cb42ac10cf41958e1d4 | 3563      | 1053       | 4      | 4     |\n",
    "| 1437 | 8836      | 962e2976f63ec09db8361ce4a498f50ade4b1674 | 3712      | 1046       | 4      | 4     |\n",
    "| 1303 | 6307      | 79ae1f05f177ad948d18dc86a7944772c63273b9 | 3559      | 971        | 6      | 6     |\n",
    "| 1126 | 6032      | b0835c4fba3d39e25059371902b0774741e3a57d | 3562      | 748        | 5      | 5     |\n",
    "| 759  | 5355      | 91be8585fcad245e9f3a53578207c58355426583 | 3603      | 540        | 4      | 4     |\n",
    "| 743  | 5981      | 42871cf2b4deaa78cbd1c163847567685c56d44e | 3654      | 567        | 4      | 4     |\n",
    "\n",
    "Note: clusters with only two or less reads are discarded (not useful for\n",
    "*cleaving*).\n",
    "\n",
    "In the second part of the pipeline, we are going to use that table as a\n",
    "list of sequences that played the role of cluster seeds in this\n",
    "particular sample. Don’t worry, that part will be explained in details.\n",
    "\n",
    "Note: the number of layers or steps does not reflect the real pairwise\n",
    "distance (pairwise distance is usually smaller):\n",
    "\n",
    "![step vs pairwise\n",
    "distance](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/steps_vs_pairwise_distance_example.png)\n",
    "\n",
    "### loop over each pair of fastq files\n",
    "\n",
    "Finally, we search for all fastq R1 files (`find`) and we apply our\n",
    "functions (merge, trim, convert, extract, dereplicate, clusterize). If\n",
    "we remove the visual clutter, it looks like this:\n",
    "\n",
    "``` shell\n",
    "find . -name \"${FASTQ_NAME_PATTERN}\" -type f -print0 | \\\n",
    "    while IFS= read -r -d '' FORWARD ; do\n",
    "        ...\n",
    "        merge_fastq_pair\n",
    "        trim_primers\n",
    "        convert_fastq_to_fasta\n",
    "        extract_expected_error_values\n",
    "        dereplicate_fasta\n",
    "        list_local_clusters\n",
    "        ...\n",
    "    done\n",
    "```\n",
    "\n",
    "That’s it for the first part of the pipeline!\n",
    "\n",
    "The whole process is very fast, even for large datasets. It is also easy\n",
    "to distribute the computation load on many machines, if need be.\n",
    "\n",
    "### trimming and merging success rate?\n",
    "\n",
    "Read merging and primer trimming are lossy filters (reads are lost). It\n",
    "is a good practice to control the per-sample and overall yield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "\n",
    "# format as a table\n",
    "for f in *.log ; do\n",
    "    echo -en \"${f/\\.log/}\\t\"\n",
    "    awk '{if (NR == 1) {printf \"%s\\t\", $1}\n",
    "          if (NR == 2) {printf $1\"\\t\"}}' ${f}\n",
    "    grep \"Reads with adapters\" \"${f}\" | \\\n",
    "        awk '{printf $4\"\\t\"}' | tr -d \",\"\n",
    "      printf \"\\n\"\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| samples | reads | assembled | F    | R    |\n",
    "|---------|-------|-----------|------|------|\n",
    "| B010    | 3359  | 2109      | 1928 | 1877 |\n",
    "| B020    | 6987  | 4191      | 3721 | 3646 |\n",
    "| B030    | 5130  | 3371      | 3089 | 2993 |\n",
    "| B040    | 7158  | 4502      | 4033 | 3967 |\n",
    "| B050    | 2903  | 1931      | 1716 | 1341 |\n",
    "| B060    | 7494  | 4337      | 3716 | 3693 |\n",
    "| B070    | 1034  | 720       | 611  | 418  |\n",
    "| B080    | 8614  | 5703      | 5327 | 5225 |\n",
    "| B090    | 9268  | 6005      | 5702 | 5630 |\n",
    "| B100    | 11644 | 7301      | 6774 | 6740 |\n",
    "| L010    | 6484  | 4101      | 3721 | 3588 |\n",
    "| L020    | 3656  | 2392      | 2195 | 2065 |\n",
    "| L030    | 3372  | 2084      | 1888 | 1766 |\n",
    "| L040    | 7981  | 5127      | 4710 | 4654 |\n",
    "| L050    | 8092  | 5377      | 5060 | 4996 |\n",
    "| L060    | 7789  | 5167      | 4707 | 4616 |\n",
    "| L070    | 7215  | 4777      | 4352 | 4292 |\n",
    "| L080    | 3577  | 2207      | 1996 | 1917 |\n",
    "| L090    | 7210  | 4291      | 3991 | 3914 |\n",
    "| L100    | 8502  | 5470      | 4947 | 4872 |\n",
    "\n",
    "A yield above 80% is good. Below 60%, something might be wrong with your\n",
    "run or your biological material (bad chemistry, target sequence too\n",
    "long, non-specific amplification, wrong primer sequences, etc.).\n",
    "\n",
    "Part 1¾: taxonomic references\n",
    "-----------------------------\n",
    "\n",
    "Before tackling the second part of the pipeline, we need to prepare our\n",
    "reference database that is going to be used for the taxonomic assignment\n",
    "of our environmental sequences.\n",
    "\n",
    "We are working with 18S V4 amplicons. We could use\n",
    "[Silva](http://www.arb-silva.de/) SSU, but\n",
    "[PR2](https://github.com/pr2database/pr2database), the Protist Ribosomal\n",
    "Reference database, is a well-curated and eukaryote specific database of\n",
    "SSU (18S) references. Let’s use the latest version (5.0), published\n",
    "earlier this month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./references/\n",
    "\n",
    "revcomp() {\n",
    "    # reverse-complement a DNA/RNA IUPAC string\n",
    "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
    "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
    "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
    "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
    "}\n",
    "\n",
    "## download PR2 (UTAX version)\n",
    "declare -r URL=\"https://github.com/pr2database/pr2database/releases/download\"\n",
    "declare -r VERSION=\"5.0.0\"\n",
    "declare -r SOURCE=\"pr2_version_${VERSION}_SSU_UTAX.fasta\"\n",
    "[[ -e \"${SOURCE}.gz\" ]] || wget \"${URL}/v${VERSION}/${SOURCE}.gz\"\n",
    "\n",
    "## search for non-ASCII characters\n",
    "zgrep --color='auto' -P -n '[^\\x00-\\x7F]' \"${SOURCE}.gz\"\n",
    "\n",
    "## extract the V4 region (primers from Stoeck et al. 2010)\n",
    "declare -r PRIMER_F=\"CCAGCASCYGCGGTAATTCC\"\n",
    "declare -r PRIMER_R=\"ACTTTCGTTCTTGATYRA\"\n",
    "declare -r ANTI_PRIMER_R=\"$(revcomp \"${PRIMER_R}\")\"\n",
    "declare -r OUTPUT=\"${SOURCE/_UTAX*/}_${PRIMER_F}_${PRIMER_R}.fas\"\n",
    "declare -r LOG=\"${OUTPUT/.fas/.log}\"\n",
    "declare -r MIN_LENGTH=\"32\"\n",
    "declare -r ERROR_RATE=\"0.2\"\n",
    "declare -r MIN_F=$(( ${#PRIMER_F} * 1 / 3 ))\n",
    "declare -r MIN_R=$(( ${#PRIMER_R} * 1 / 3 ))\n",
    "declare -r OPTIONS=\"--minimum-length ${MIN_LENGTH} --discard-untrimmed --error-rate ${ERROR_RATE}\"\n",
    "declare -r CUTADAPT=\"$(which cutadapt) ${OPTIONS}\"\n",
    "\n",
    "zcat \"${SOURCE}.gz\" | \\\n",
    "    dos2unix | \\\n",
    "    sed '/^>/ s/;tax=k:/ /\n",
    "         /^>/ s/,[dpcofgs]:/|/g\n",
    "         /^>/ ! s/U/T/g' | \\\n",
    "     ${CUTADAPT} \\\n",
    "         --revcomp \\\n",
    "         --front \"${PRIMER_F}\" \\\n",
    "         --overlap \"${MIN_F}\" - 2> \"${LOG}\" | \\\n",
    "         ${CUTADAPT} \\\n",
    "         --adapter \"${ANTI_PRIMER_R}\" \\\n",
    "         --overlap \"${MIN_R}\" - > \"${OUTPUT}\" 2>> \"${LOG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won’t go into details here, but you might notice some familiar code\n",
    "such as `revcomp()` and two calls to `cutadapt`. The goal here is to\n",
    "download and trim the reference sequences:\n",
    "\n",
    "    reference 1: xxxxxxxx-PRIMER_F-target_region-PRIMER_R-xxxxx\n",
    "    reference 2:            IMER_F-target_region-PRIMER_R-xxxxxxxxxxx\n",
    "    reference 3:                    arget-region-PRIMER_R-xxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "Discard references that do not contain our target region (flanked by our\n",
    "primers), and trim the primers. The reference dataset is now much\n",
    "smaller, and limited to our target region:\n",
    "\n",
    "    reference 1:                   target_region\n",
    "    reference 2:                   target_region\n",
    "\n",
    "Note: while preparing this, I’ve found a small bug in the latest PR2\n",
    "release ([weird character in one species\n",
    "name](https://github.com/pr2database/pr2database/issues/37))\n",
    "\n",
    "### subsample PR2\n",
    "\n",
    "With a complete PR2 reference dataset, taxonomic assignment would take\n",
    "more than an hour on a Google colab instance (that’s the most\n",
    "computationally intensive step in whole the pipeline). To speed up\n",
    "things, I provide a PR2 subset with exactly what we need for the\n",
    "Neotropical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./references/\n",
    "\n",
    "URL=\"https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/references\"\n",
    "LIST=\"useful_references.list\"\n",
    "TRIMMED_PR2=\"pr2_version_5.0.0_SSU_CCAGCASCYGCGGTAATTCC_ACTTTCGTTCTTGATYRA.fas\"\n",
    "\n",
    "wget --continue \"${URL}/${LIST}\"\n",
    "\n",
    "grep \\\n",
    "    --no-group-separator \\\n",
    "    --after-context=1 \\\n",
    "    --fixed-strings \\\n",
    "    --file \"${LIST}\" \\\n",
    "    \"${TRIMMED_PR2}\" > \"${TRIMMED_PR2/\\.fas/_subset.fas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: from fasta files to an annotated occurrence table\n",
    "---------------------------------------------------------\n",
    "\n",
    "![pipeline\n",
    "overview](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/diapo_pipeline_final_colour.png)\n",
    "\n",
    "The pipeline is divided into two parts. A first part where each sample\n",
    "is processed individually. And a second part where all samples are\n",
    "pooled to produce an occurrence table.\n",
    "\n",
    "In this second part of the pipeline, we will: 1. pool metadata (amplicon\n",
    "distributions, local clustering results), 1. pool fasta samples and\n",
    "dereplicate, 1. clusterize the whole dataset, 1. detect chimeras, 1.\n",
    "*cleave* (separate un-correlated sub-clusters; complement to `lulu`), 1.\n",
    "assign to taxonomic references, 1. build a filtered occurrence table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./results/\n",
    "export LC_ALL=C\n",
    "\n",
    "## ------------------------------------------------------------ define variables\n",
    "declare -r PROJECT=\"Neotropical_soils_18S_V4\"\n",
    "declare -r DATA_FOLDER=\"../data/\"\n",
    "declare -r SWARM=\"$(which swarm)\"  # swarm 3.0 or more recent\n",
    "declare -r VSEARCH=\"$(which vsearch)\"  # vsearch 2.21 or more recent\n",
    "declare -ri THREADS=2\n",
    "declare -ri RESOLUTION=1\n",
    "declare -ri FILTER=2\n",
    "declare -r SRC=\"../src\"\n",
    "declare -r OTU_CLEAVER=\"OTU_cleaver.py\"\n",
    "declare -r OTU_TABLE_BUILDER=\"OTU_contingency_table_filtered.py\"\n",
    "declare -r OTU_TABLE_UPDATER=\"OTU_table_updater.py\"\n",
    "declare -r STAMPA_MERGE=\"stampa_merge.py\"\n",
    "declare -r DATABASE=\"../references/pr2_version_5.0.0_SSU_CCAGCASCYGCGGTAATTCC_ACTTTCGTTCTTGATYRA_subset.fas\"\n",
    "\n",
    "N_SAMPLES=$(find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "                -type f ! -empty -print0 | tr -d -c '\\0' | wc --chars)\n",
    "FINAL_FASTA=\"${PROJECT}_${N_SAMPLES}_samples.fas\"\n",
    "QUALITY_FILE=\"${FINAL_FASTA%.*}.qual\"\n",
    "DISTRIBUTION_FILE=\"${FINAL_FASTA%.*}.distr\"\n",
    "POTENTIAL_SUB_SEEDS=\"${FINAL_FASTA%.*}_per_sample_OTUs.stats\"\n",
    "LOG=\"${FINAL_FASTA%.*}.log\"\n",
    "OUTPUT_SWARMS=\"${FINAL_FASTA%.*}_${RESOLUTION}f.swarms\"\n",
    "OUTPUT_LOG=\"${FINAL_FASTA%.*}_${RESOLUTION}f.log\"\n",
    "OUTPUT_STATS=\"${FINAL_FASTA%.*}_${RESOLUTION}f.stats\"\n",
    "OUTPUT_STRUCT=\"${FINAL_FASTA%.*}_${RESOLUTION}f.struct\"\n",
    "OUTPUT_REPRESENTATIVES=\"${FINAL_FASTA%.*}_${RESOLUTION}f_representatives.fas\"\n",
    "TAXONOMIC_ASSIGNMENTS=\"${OUTPUT_REPRESENTATIVES%.*}.results\"\n",
    "UCHIME_RESULTS=\"${OUTPUT_REPRESENTATIVES%.*}.uchime\"\n",
    "UCHIME_LOG=\"${OUTPUT_REPRESENTATIVES%.*}.log\"\n",
    "OTU_TABLE=\"${FINAL_FASTA%.*}.OTU.filtered.cleaved.table\"\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------- global clustering\n",
    "echo \"run global clustering and chimera detection...\"\n",
    "\n",
    "## Build expected error file\n",
    "find \"${DATA_FOLDER}\" -name \"*.qual\" \\\n",
    "    -type f ! -empty -print0 | \\\n",
    "    sort -k3,3n -k1,1d -k2,2n --merge --files0-from=- | \\\n",
    "    uniq --check-chars=40 > \"${QUALITY_FILE}\" &\n",
    "\n",
    "\n",
    "## Build distribution file (sequence <-> sample relations)\n",
    "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "    -type f ! -empty -execdir grep -H \"^>\" '{}' \\; | \\\n",
    "    sed 's/.*\\/// ; s/\\.fas:>/\\t/ ; s/;size=/\\t/ ; s/;$//' | \\\n",
    "    awk 'BEGIN {FS = OFS = \"\\t\"} {print $2, $1, $3}' > \"${DISTRIBUTION_FILE}\" &\n",
    "\n",
    "\n",
    "## list all cluster seeds of size > 2\n",
    "find \"${DATA_FOLDER}\" -name \"*.stats\" \\\n",
    "    -type f ! -empty -execdir grep --with-filename \"\" '{}' \\; | \\\n",
    "    sed 's/^\\.\\/// ; s/\\.stats:/\\t/' > \"${POTENTIAL_SUB_SEEDS}\" &\n",
    "\n",
    "\n",
    "## global dereplication\n",
    "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "    -type f ! -empty -execdir cat '{}' + | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --derep_fulllength - \\\n",
    "        --quiet \\\n",
    "        --sizein \\\n",
    "        --sizeout \\\n",
    "        --log \"${LOG}\" \\\n",
    "        --fasta_width 0 \\\n",
    "        --output \"${FINAL_FASTA}\"\n",
    "\n",
    "\n",
    "## clustering (swarm 3 or more recent)\n",
    "\"${SWARM}\" \\\n",
    "    --differences \"${RESOLUTION}\" \\\n",
    "    --fastidious \\\n",
    "    --usearch-abundance \\\n",
    "    --threads \"${THREADS}\" \\\n",
    "    --internal-structure \"${OUTPUT_STRUCT}\" \\\n",
    "    --output-file \"${OUTPUT_SWARMS}\" \\\n",
    "    --statistics-file \"${OUTPUT_STATS}\" \\\n",
    "    --seeds \"${OUTPUT_REPRESENTATIVES}\" \\\n",
    "    \"${FINAL_FASTA}\" 2> \"${OUTPUT_LOG}\"\n",
    "\n",
    "\n",
    "## fake taxonomic assignment\n",
    "grep \"^>\" \"${OUTPUT_REPRESENTATIVES}\" | \\\n",
    "    sed --regexp-extended 's/^>//\n",
    "                           s/;size=/\\t/\n",
    "                           s/;?$/\\t0.0\\tNA\\tNA/' > \"${TAXONOMIC_ASSIGNMENTS}\"\n",
    "\n",
    "\n",
    "## chimera detection\n",
    "## discard sequences with an abundance lower than FILTER\n",
    "## and search for chimeras\n",
    "\"${VSEARCH}\" \\\n",
    "    --fastx_filter \"${OUTPUT_REPRESENTATIVES}\"  \\\n",
    "    --quiet \\\n",
    "    --minsize \"${FILTER}\" \\\n",
    "    --fastaout - | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --uchime_denovo - \\\n",
    "        --quiet \\\n",
    "        --uchimeout \"${UCHIME_RESULTS}\" \\\n",
    "        2> \"${UCHIME_LOG}\"\n",
    "\n",
    "\n",
    "## ------------------------------------------------------------------- cleaving\n",
    "echo\n",
    "echo \"run cleaving...\"\n",
    "\n",
    "## split OTUs\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_CLEAVER}\" \\\n",
    "    --global_stats \"${OUTPUT_STATS}\" \\\n",
    "    --per_sample_stats \"${POTENTIAL_SUB_SEEDS}\" \\\n",
    "    --struct \"${OUTPUT_STRUCT}\" \\\n",
    "    --swarms \"${OUTPUT_SWARMS}\" \\\n",
    "    --fasta \"${FINAL_FASTA}\"\n",
    "\n",
    "## fake taxonomic assignment\n",
    "grep \"^>\" \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
    "    sed --regexp-extended 's/^>//\n",
    "                           s/;size=/\\t/\n",
    "                           s/;?$/\\t0.0\\tNA\\tNA/' > \"${TAXONOMIC_ASSIGNMENTS}2\"\n",
    "\n",
    "## chimera detection (only down to the smallest newly cleaved OTU)\n",
    "LOWEST_ABUNDANCE=$(sed --regexp-extended --quiet \\\n",
    "    '/^>/ s/.*;size=([0-9]+);?/\\1/p' \\\n",
    "    \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
    "    sort --numeric-sort | \\\n",
    "    head -n 1)\n",
    "\n",
    "# sort and filter by abundance (default to an abundance of 1), search\n",
    "# for chimeras\n",
    "cat \"${OUTPUT_REPRESENTATIVES}\" \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --sortbysize - \\\n",
    "        --quiet \\\n",
    "        --sizein \\\n",
    "        --minsize ${LOWEST_ABUNDANCE:-1} \\\n",
    "        --sizeout \\\n",
    "        --output - | \\\n",
    "        \"${VSEARCH}\" \\\n",
    "            --uchime_denovo - \\\n",
    "            --quiet \\\n",
    "            --uchimeout \"${UCHIME_RESULTS}2\" \\\n",
    "            2> \"${OUTPUT_REPRESENTATIVES%.*}.log2\"\n",
    "\n",
    "unset LOWEST_ABUNDANCE\n",
    "\n",
    "\n",
    "## ------------------------------------------------------------ first OTU table\n",
    "echo\n",
    "echo \"build first OTU table...\"\n",
    "\n",
    "# build OTU table\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_TABLE_BUILDER}\" \\\n",
    "    --representatives <(cat \"${OUTPUT_REPRESENTATIVES}\"{,2}) \\\n",
    "    --stats <(cat \"${OUTPUT_STATS}\"{,2}) \\\n",
    "    --swarms <(cat \"${OUTPUT_SWARMS}\"{,2}) \\\n",
    "    --chimera <(cat \"${UCHIME_RESULTS}\"{,2}) \\\n",
    "    --quality \"${QUALITY_FILE}\" \\\n",
    "    --assignments <(cat \"${TAXONOMIC_ASSIGNMENTS}\"{2,}) \\\n",
    "    --distribution \"${DISTRIBUTION_FILE}\" > \"${OTU_TABLE}\"\n",
    "\n",
    "\n",
    "# extract fasta sequences from OTU table\n",
    "awk 'NR > 1 {printf \">\"$4\";size=\"$2\";\\n\"$10\"\\n\"}' \"${OTU_TABLE}\" \\\n",
    "    > \"${OTU_TABLE/.table/.fas}\"\n",
    "\n",
    "\n",
    "\n",
    "## ------------------------------------------------------- taxonomic assignment\n",
    "echo\n",
    "echo \"taxonomic assignment...\"\n",
    "\n",
    "# search for best hits\n",
    "${VSEARCH} \\\n",
    "    --usearch_global \"${OTU_TABLE/.table/.fas}\" \\\n",
    "    --db ${DATABASE} \\\n",
    "    --quiet \\\n",
    "    --threads ${THREADS} \\\n",
    "    --dbmask none \\\n",
    "    --qmask none \\\n",
    "    --rowlen 0 \\\n",
    "    --notrunclabels \\\n",
    "    --userfields query+id1+target \\\n",
    "    --maxaccepts 0 \\\n",
    "    --maxrejects 0 \\\n",
    "    --top_hits_only \\\n",
    "    --output_no_hits \\\n",
    "    --id 0.5 \\\n",
    "    --iddef 1 \\\n",
    "    --userout - | \\\n",
    "    sed 's/;size=/_/ ; s/;//' > hits.representatives\n",
    "\n",
    "# in case of multi-best hit, find the last-common ancestor\n",
    "python3 ${SRC}/${STAMPA_MERGE} $(pwd)\n",
    "\n",
    "# sort by decreasing abundance\n",
    "sort -k2,2nr -k1,1d results.representatives > \"${OTU_TABLE/.table/.results}\"\n",
    "\n",
    "# clean-up\n",
    "rm hits.representatives results.representatives\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------- update OTU table\n",
    "echo\n",
    "echo \"build final OTU table...\"\n",
    "NEW_TABLE=$(mktemp)\n",
    "\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_TABLE_UPDATER}\" \\\n",
    "    --old_otu_table \"${OTU_TABLE}\" \\\n",
    "    --new_taxonomy \"${OTU_TABLE/.table/.results}\" \\\n",
    "    --new_otu_table \"${NEW_TABLE}\"\n",
    "\n",
    "# fix OTU sorting\n",
    "(head -n 1 \"${NEW_TABLE}\"\n",
    "    tail -n +2 \"${NEW_TABLE}\" | \\\n",
    "    sort -k2,2nr | \\\n",
    "    nl --number-format='ln' --number-width=1 | \\\n",
    "    cut --complement -f 2\n",
    ") > \"${OTU_TABLE}2\"\n",
    "\n",
    "# clean up\n",
    "rm \"${NEW_TABLE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pool expected error values\n",
    "\n",
    "So far we had an expected error value for each unique amplicon in each\n",
    "sample. Here, we group them to get an expected error value for each\n",
    "unique amplicon in the bioproject:\n",
    "\n",
    "``` shell\n",
    "### Build expected error file\n",
    "find \"${DATA_FOLDER}\" -name \"*.qual\" \\\n",
    "    -type f ! -empty -print0 | \\\n",
    "    sort -k3,3n -k1,1d -k2,2n --merge --files0-from=- | \\\n",
    "    uniq --check-chars=40 > \"${QUALITY_FILE}\" &\n",
    "```\n",
    "\n",
    "The result is a table with three-columns, sorted by increasing length:\n",
    "1. amplicon name, 1. lowest expected error observed for that amplicon,\n",
    "1. amplicon length\n",
    "\n",
    "    964ab48de96fa2997896c4d80db52e412d5da64d 0.002542 32\n",
    "    f55456130e6d1a43af8dd89ee58432273d872775 0.002542 32\n",
    "    4fbd2164d6898278c5981f3a18701d9b44e18a5a 0.002621 33\n",
    "    88885c6241c729d020f8fffc4f7505bb599e5bf8 0.002621 33\n",
    "    c4505959d778ac5eb94f4971fe902f9c34c48763 0.002701 34\n",
    "    dca48a272d66c555b06317af2472455db8140237 0.002780 35\n",
    "    9f2cd2223ed79292d3317fc50b0060bb1a726296 0.002939 37\n",
    "    7ceba3f3dee8aaaeac5079b5ee0364e38bf5006e 0.3192 38\n",
    "    86127aa0eb63c3a047705c09095b2b12a88856d6 0.004934 38\n",
    "    94b7001f660573b59e33fea8e53da0afb51d9f69 0.007367 38\n",
    "    ...\n",
    "\n",
    "### distribution\n",
    "\n",
    "For each unique amplicon in the dataset, record the samples where it\n",
    "occurs, and its abundance in that sample (number of reads):\n",
    "\n",
    "``` shell\n",
    "## Build distribution file (sequence <-> sample relations)\n",
    "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "    -type f ! -empty -execdir grep -H \"^>\" '{}' \\; | \\\n",
    "    sed 's/.*\\/// ; s/\\.fas:>/\\t/ ; s/;size=/\\t/ ; s/;$//' | \\\n",
    "    awk 'BEGIN {FS = OFS = \"\\t\"} {print $2, $1, $3}' > \"${DISTRIBUTION_FILE}\" &\n",
    "```\n",
    "\n",
    "The results is a three-column table:\n",
    "\n",
    "``` text\n",
    "f414cfec8c1cc3973e95e67cff46299a00e8368a    S001    7369\n",
    "16b79e33e7897ca08ecaa282dc4b4ba1e6d6b460    S001    679\n",
    "ea5b349a8215a8b8ca2de29f0a33087b7c7d5e77    S001    458\n",
    "1f51f06217fa2ac348b50fea587702e29bfe1f1c    S001    315\n",
    "288366fe126296cb6c6aec488dde3b25a6385d5f    S001    243\n",
    "f59d28a1e80d582e07d0b224f07ac6e360a8acef    S001    154\n",
    "e74f16e7aaeaae903b09ddb6d9f16c66acd47f9a    S001    139\n",
    "be8e83ba27fa599c1eccb416e06131870ba101e6    S001    119\n",
    "324f9ad3b7651dbbca36abd62ed9c39238db12c1    S001    98\n",
    "2ba9bb0fe66318a974db3a580b23b3b1691f4a54    S001    87\n",
    "```\n",
    "\n",
    "### local cluster representatives\n",
    "\n",
    "Earlier in this analysis, we used `swarm` to make a list of clusters and\n",
    "cluster seeds present in each sample. Here, we are simply pooling these\n",
    "per-sample lists into a single file, adding the sample name:\n",
    "\n",
    "``` shell\n",
    "## list all cluster seeds of size > 2\n",
    "find \"${DATA_FOLDER}\" -name \"*.stats\" \\\n",
    "    -type f ! -empty -execdir grep --with-filename \"\" '{}' \\; | \\\n",
    "    sed 's/^\\.\\/// ; s/\\.stats:/\\t/' > \"${POTENTIAL_SUB_SEEDS}\" &\n",
    "```\n",
    "\n",
    "This information will be used during the cleaving step:\n",
    "\n",
    "| sample | uniq | abundance | seed                                     | abundance | singletons | layers | steps |\n",
    "|--------|------|-----------|------------------------------------------|-----------|------------|--------|-------|\n",
    "| L010   | 5897 | 22010     | 3db68d2f77252793f23d7089d0d4103eb8942dcb | 3612      | 4600       | 8      | 8     |\n",
    "| L010   | 5528 | 15548     | 45211af6b7811bf45b5d3694054b800b5b13efd4 | 3728      | 4488       | 9      | 9     |\n",
    "| L010   | 3058 | 13452     | 47e639615ad19c8dededae45f38beb52c4e9861d | 3542      | 2252       | 9      | 9     |\n",
    "| L010   | 2172 | 8268      | 307ab3d7f513adc74854dd72e817fe02f7601db2 | 3722      | 1639       | 8      | 8     |\n",
    "| L010   | 1453 | 6754      | e52a67c43758a5f9a39b4cb42ac10cf41958e1d4 | 3563      | 1053       | 4      | 4     |\n",
    "| L010   | 1437 | 8836      | 962e2976f63ec09db8361ce4a498f50ade4b1674 | 3712      | 1046       | 4      | 4     |\n",
    "| L010   | 1303 | 6307      | 79ae1f05f177ad948d18dc86a7944772c63273b9 | 3559      | 971        | 6      | 6     |\n",
    "| L010   | 1126 | 6032      | b0835c4fba3d39e25059371902b0774741e3a57d | 3562      | 748        | 5      | 5     |\n",
    "| L010   | 759  | 5355      | 91be8585fcad245e9f3a53578207c58355426583 | 3603      | 540        | 4      | 4     |\n",
    "| L010   | 743  | 5981      | 42871cf2b4deaa78cbd1c163847567685c56d44e | 3654      | 567        | 4      | 4     |\n",
    "\n",
    "### pool fasta entries\n",
    "\n",
    "Metabarcoding samples often share some of their amplicons. Find all the\n",
    "non-empty fasta files created during the first part of the pipeline, and\n",
    "dereplicate them. This is a fast and lossless reduction of the volume of\n",
    "our data:\n",
    "\n",
    "``` shell\n",
    "## global dereplication\n",
    "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "    -type f ! -empty -execdir cat '{}' + | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --derep_fulllength - \\\n",
    "        --quiet \\\n",
    "        --sizein \\\n",
    "        --sizeout \\\n",
    "        --log \"${LOG}\" \\\n",
    "        --fasta_width 0 \\\n",
    "        --output \"${FINAL_FASTA}\"\n",
    "```\n",
    "\n",
    "Note: this time we use the option `--sizein` to take into account the\n",
    "abundance obtained during sample-level dereplications.\n",
    "\n",
    "For instance, this fasta entries coming from different samples:\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=3\n",
    "    AAAA\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=2\n",
    "    AAAA\n",
    "\n",
    "will be dereplicated into this one:\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=5\n",
    "    AAAA\n",
    "\n",
    "Dereplication is strongly recommended before clustering with `swarm`!\n",
    "\n",
    "### clustering\n",
    "\n",
    "Global clustering with `swarm`.\n",
    "\n",
    "Why do we need clustering/denoising?\n",
    "\n",
    "Amplification and sequencing are **noisy**, and the level of noise is\n",
    "directly correlated to the initial abundance of your target sequence:\n",
    "\n",
    "![seed vs\n",
    "cloud](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/seed_vs_cloud.png)\n",
    "\n",
    "The number of microvariants (errors?) around a seed (true sequence?) is\n",
    "a function of the abundance of the seed. Abundant seeds will be at the\n",
    "center of a dense and large cloud of microvariants.\n",
    "\n",
    "`swarm` will use that. With default parameters, it will generate all\n",
    "possible microvariants of an amplicon and simply check if these\n",
    "microvariants are present in the dataset and capture them if they are.\n",
    "Iteratively.\n",
    "\n",
    "I recommend to use a local difference of one (`d = 1`) with the\n",
    "`--fastidious` option. A local difference of one is `swarm`’s default\n",
    "and recommended setting. Combining it with the `--fastidious` option is\n",
    "a very good compromise between taxonomic resolution and molecular\n",
    "diversity inflation (too many clusters):\n",
    "\n",
    "``` shell\n",
    "## clustering (swarm 3 or more recent)\n",
    "\"${SWARM}\" \\\n",
    "    --differences \"${RESOLUTION}\" \\\n",
    "    --fastidious \\\n",
    "    --usearch-abundance \\\n",
    "    --threads \"${THREADS}\" \\\n",
    "    --internal-structure \"${OUTPUT_STRUCT}\" \\\n",
    "    --output-file \"${OUTPUT_SWARMS}\" \\\n",
    "    --statistics-file \"${OUTPUT_STATS}\" \\\n",
    "    --seeds \"${OUTPUT_REPRESENTATIVES}\" \\\n",
    "    \"${FINAL_FASTA}\" 2> \"${OUTPUT_LOG}\"\n",
    "```\n",
    "\n",
    "`swarm` outputs clusters and cluster representatives in fasta format.\n",
    "`swarm` can also output interesting stats and a description of the\n",
    "internal structure of the clusters. We are going to use all these files.\n",
    "\n",
    "`swarm`’s log contains a lot of information. The most important ones are\n",
    "the number of clusters, the number of unique sequences in the largest\n",
    "cluster, and the memory consumption. In its current acceptation, the\n",
    "term OTU designates 97%-based clusters. In that sense, `swarm` produces\n",
    "ASVs (down to a one-nucleotide resolution after the cleaving step).\n",
    "\n",
    "### Chimera detection\n",
    "\n",
    "Chimeras are very frequent in metabarcoding dataset. Their frequency\n",
    "increases with the number of PCR cycles, and it is important to detect\n",
    "and remove them. The dominant algorithm today is `uchime` (citation),\n",
    "and we are using its `vsearch` implementation.\n",
    "\n",
    "To reduce the computational load, we are working with cluster\n",
    "representatives, rather than working with the whole fasta dataset.\n",
    "Similarly, we are also not checking the chimeric status of clusters with\n",
    "only one read (*singletons*):\n",
    "\n",
    "``` shell\n",
    "## chimera detection\n",
    "## discard sequences with an abundance lower than FILTER\n",
    "## and search for chimeras\n",
    "\"${VSEARCH}\" \\\n",
    "    --fastx_filter \"${OUTPUT_REPRESENTATIVES}\"  \\\n",
    "    --quiet \\\n",
    "    --minsize \"${FILTER}\" \\\n",
    "    --fastaout - | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --uchime_denovo - \\\n",
    "        --quiet \\\n",
    "        --uchimeout \"${UCHIME_RESULTS}\" \\\n",
    "        2> \"${UCHIME_LOG}\"\n",
    "```\n",
    "\n",
    "Here is `vsearch`’s report:\n",
    "\n",
    "    vsearch v2.22.1_linux_x86_64, 62.7GB RAM, 8 cores\n",
    "    https://github.com/torognes/vsearch\n",
    "\n",
    "    Reading file - 100%\n",
    "    773659 nt in 2091 seqs, min 34, max 503, avg 370\n",
    "    Masking 100%\n",
    "    Sorting by abundance 100%\n",
    "    Counting k-mers 100%\n",
    "    Detecting chimeras 100%\n",
    "    Found 399 (19.1%) chimeras, 1666 (79.7%) non-chimeras,\n",
    "    and 26 (1.2%) borderline sequences in 2091 unique sequences.\n",
    "    Taking abundance information into account, this corresponds to\n",
    "    3869 (6.1%) chimeras, 59815 (93.6%) non-chimeras,\n",
    "    and 221 (0.3%) borderline sequences in 63905 total sequences.\n",
    "\n",
    "Note: a high number of chimeras. This is usual for long markers such as\n",
    "18S V4. The data loss is small in terms of reads (6.4% in total).\n",
    "\n",
    "Chimera detection is complex and should be a priority research field.\n",
    "There is a new chimera detection algorithm in preparation for vsearch!\n",
    "\n",
    "### cleaving\n",
    "\n",
    "This is a procedure I use to post-process `swarm`’s clustering results,\n",
    "and to improve our taxonomic resolution down to one nucleotide, where\n",
    "necessary.\n",
    "\n",
    "Where `lulu` groups similar clusters that have the same distribution\n",
    "patterns (co-occur in the same samples), cleaving does the complement\n",
    "operation. Cleaving separates cluster sub-parts (similar sequences) that\n",
    "do not co-occur.\n",
    "\n",
    "For example, a cluster with two sub-parts A and B:\n",
    "\n",
    "| s1  | s2  | s3  | s4  | s5  | s6  |     |\n",
    "|-----|-----|-----|-----|-----|-----|-----|\n",
    "| A   | 9   | 7   | 9   | 0   | 0   | 0   |\n",
    "| B   | 0   | 0   | 0   | 7   | 8   | 9   |\n",
    "\n",
    "A and B do not co-occur, there is some ecological signal here. Cleaving\n",
    "will create two separated clusters A and B.\n",
    "\n",
    "![cleaved\n",
    "cluster](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/grasp_nodD_764_samples_1_OTU_6.pdf)\n",
    "\n",
    "Cleaving is a fast operation, even for extremely large datasets. No\n",
    "artificial diversity inflation, resolution only when necessary.\n",
    "\n",
    "Cleaving has a visible effect on abundance vs. cloud distributions:\n",
    "\n",
    "![cleaved\n",
    "cluster](https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/images/grasp_nodD_764_samples_1f.stats.before_after_OTU_breaking.png)\n",
    "\n",
    "Some outliers (above the average) are separated into sub-components that\n",
    "are much closer to the average abundance vs. cloud ratio.\n",
    "\n",
    "Note : after cleaving, it is necessary to perform a new chimera check.\n",
    "\n",
    "### build filtered occurrence table\n",
    "\n",
    "Pool all the information produced above, and build an occurrence table:\n",
    "\n",
    "``` shell\n",
    "# build OTU table\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_TABLE_BUILDER}\" \\\n",
    "    --representatives <(cat \"${OUTPUT_REPRESENTATIVES}\"{,2}) \\\n",
    "    --stats <(cat \"${OUTPUT_STATS}\"{,2}) \\\n",
    "    --swarms <(cat \"${OUTPUT_SWARMS}\"{,2}) \\\n",
    "    --chimera <(cat \"${UCHIME_RESULTS}\"{,2}) \\\n",
    "    --quality \"${QUALITY_FILE}\" \\\n",
    "    --assignments <(cat \"${TAXONOMIC_ASSIGNMENTS}\"{2,}) \\\n",
    "    --distribution \"${DISTRIBUTION_FILE}\" > \"${OTU_TABLE}\"\n",
    "```\n",
    "\n",
    "This python script will also filter out chimeras, clusters with a low\n",
    "quality seed, and clusters with a low abundance and seen in only one or\n",
    "two samples:\n",
    "\n",
    "``` python\n",
    "        if (\n",
    "            chimera_status == \"N\"\n",
    "            and high_quality <= EE_threshold\n",
    "            and (abundance >= 3 or spread >= 2)\n",
    "        ):\n",
    "```\n",
    "\n",
    "Note: eliminating chimeras before taxonomic assignment is a matter of\n",
    "discussion. If a chimera is 100% identical to a reference sequence, then\n",
    "it is a false positive (not a chimera) or a sign that the matching\n",
    "reference should be investigated.\n",
    "\n",
    "### Taxonomy: last-common ancestor\n",
    "\n",
    "Brute-force approach! This is the slowest step in the pipeline, by far.\n",
    "It can easily be distributed on a large cluster of computers and be done\n",
    "under 20 minutes, even for a large dataset. This is the method I use in\n",
    "my own projects, but I admit it is not the most efficient, and other\n",
    "methods should be explored (naive Bayesian classifier, Markov model,\n",
    "etc.):\n",
    "\n",
    "``` shell\n",
    "# search for best hits\n",
    "${VSEARCH} \\\n",
    "    --usearch_global \"${OTU_TABLE/.table/.fas}\" \\\n",
    "    --db ${DATABASE} \\\n",
    "    --quiet \\\n",
    "    --threads ${THREADS} \\\n",
    "    --dbmask none \\\n",
    "    --qmask none \\\n",
    "    --rowlen 0 \\\n",
    "    --notrunclabels \\\n",
    "    --userfields query+id1+target \\\n",
    "    --maxaccepts 0 \\\n",
    "    --maxrejects 0 \\\n",
    "    --top_hits_only \\\n",
    "    --output_no_hits \\\n",
    "    --id 0.5 \\\n",
    "    --iddef 1 \\\n",
    "    --userout - | \\\n",
    "    sed 's/;size=/_/ ; s/;//' > hits.representatives\n",
    "```\n",
    "\n",
    "    1462274 nt in 4001 seqs, min 87, max 466, avg 365\n",
    "    Counting k-mers 100%\n",
    "    Creating k-mer index 100%\n",
    "    Searching 100%\n",
    "    Matching unique query sequences: 1272 of 1358 (93.67%)\n",
    "\n",
    "Results:\n",
    "\n",
    "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    GU319785.1.833_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|Cryptosporidium13|Cryptosporidium13_muris\n",
    "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    EU156446.1.816_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|Cryptosporidium15|Cryptosporidium15_sp.\n",
    "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    JQ413358.1.796_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|Cryptosporidium13|Cryptosporidium13_muris\n",
    "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    AB000647.1.1737_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
    "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    JQ698930.1.1731_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
    "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    U00974.1.1719_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
    "    ...\n",
    "\n",
    "The advantage of this method is the clarity of the results. Since both\n",
    "query and reference cover the same genomic region (in-between our\n",
    "forward and reverse primers), then we can use a simple, easy to\n",
    "interpret similarity definition.\n",
    "\n",
    "As you can see a given environmental amplicon can be assigned to more\n",
    "than one reference, and these references can point to different taxa. To\n",
    "reflect that uncertain taxonomic assignment, each environmental amplicon\n",
    "is assigned to the last-common ancestor of its closest references:\n",
    "\n",
    "``` shell\n",
    "# in case of multi-best hit, find the last-common ancestor\n",
    "python3 ${SRC}/${STAMPA_MERGE} $(pwd)\n",
    "```\n",
    "\n",
    "Resulting in:\n",
    "\n",
    "    8e75b9c004b4188ea41b6897b5bfb12af6f36796_3368   77.3    GU319785.1.833_U,EU156446.1.816_U,JQ413358.1.796_U Eukaryota|TSAR|Alveolata-Apicomplexa|Gregarinomorphea|Cryptogregarinorida|Cryptosporidiidae|*|*\n",
    "\n",
    "    ff365793fda117a3c87f972342d97c8738555133_3419   96.8    AB000647.1.1737_U,JQ698930.1.1731_U,U00974.1.1719_U Eukaryota|Obazoa|Opisthokonta-Fungi|Ascomycota|Saccharomycotina|Saccharomycetales|Galactomyces|Galactomyces_geotrichum\n",
    "    ...\n",
    "\n",
    "If an environmental sequence is equidistant to several reference\n",
    "sequences, the it is assigned to the last-common ancestor (part of the\n",
    "taxonomic path that is common) of the references. Conflicting\n",
    "assignments are replaced with stars (`*`).\n",
    "\n",
    "This method is great for finding errors in reference databases. For\n",
    "example, the third most-abundant cluster in our dataset\n",
    "(`5fc4b348c56e446f6efdd9ee50d6388b8f691915`, with a total of 3,125\n",
    "reads) is assigned to `*|*|*|*|*|*|*|*` with a similarity of 100%.\n",
    "\n",
    "That’s weird, let’s have a look.\n",
    "\n",
    "This cluster has a 100% similarity with 244 reference sequences: - 242\n",
    "references are from an Embryophyceae (*Zea mays*), - 2 references are\n",
    "from an unknown Embryophyceae (Embryophyceae XXX sp.), - 1 reference is\n",
    "from a Bacteria (*Pseudooceanicola lipolyticus*, an Alphaproteobacteria)\n",
    "\n",
    "This last entry is very likely a misassigned reference sequence, and our\n",
    "environmental sequence is probably an Embryophyceae (but not maize, as\n",
    "it was collected deep into the equatorial forest).\n",
    "\n",
    "### occurrence table\n",
    "\n",
    "#### structure\n",
    "\n",
    "1.  OTU number\n",
    "2.  total number of reads\n",
    "3.  cloud (total number of unique sequences)\n",
    "4.  amplicon (identifier of the OTU representative)\n",
    "5.  length (length of the OTU representative)\n",
    "6.  abundance (abundance of the OTU representative)\n",
    "7.  chimera (is it a chimera? Yes, No, ?)\n",
    "8.  spread (number of samples where the OTU occurs)\n",
    "9.  quality (minimum expected error observed for the OTU representative,\n",
    "    divided by sequence length)\n",
    "10. sequence (sequence of the OTU representative)\n",
    "11. identity (maximum similarity of the OTU representative with\n",
    "    reference sequences)\n",
    "12. taxonomy (taxonomic assignment of the OTU representative)\n",
    "13. references (reference sequences closest to the OTU representative)\n",
    "14. sample 1\n",
    "15. sample 2\n",
    "16. sample…\n",
    "\n",
    "#### first look at the table\n",
    "\n",
    "We seen earlier that our third most-abundant cluster was assigned to the\n",
    "last-common ancestor `*|*|*|*|*|*|*|*` with a similarity of 100%. That\n",
    "sequence is probably from a plant.\n",
    "\n",
    "Our most-abundant cluster is a Fungi (Ascomycota, with 96.8% of\n",
    "similarity), as is our fourth-most abundant cluster (Basidiomycota,\n",
    "99.7%). These Fungi groups are the two most frequent in soils.\n",
    "\n",
    "A striking feature is the presence of these abundant and weakly assigned\n",
    "clusters (5 out of the 10 most-abundant clusters, 70 to 80% similarity\n",
    "with references). These clusters represent a yet-to-be-described\n",
    "molecular diversity of Apicomplexans:\n",
    "\n",
    "| Taxa                     | reads |\n",
    "|--------------------------|-------|\n",
    "| Alveolata-Apicomplexa    | 24951 |\n",
    "| Opisthokonta-Fungi       | 16508 |\n",
    "| Streptophyta             | 7417  |\n",
    "| Opisthokonta-Metazoa     | 3115  |\n",
    "| Rhizaria-Cercozoa        | 1676  |\n",
    "| Alveolata-Ciliophora     | 1463  |\n",
    "| Stramenopiles-Gyrista    | 587   |\n",
    "| Alveolata-Perkinsea      | 465   |\n",
    "| Alveolata-Dinoflagellata | 331   |\n",
    "\n",
    "#### additional filters?\n",
    "\n",
    "As mentioned above, some basic filters were applied to build this table.\n",
    "In part 1, only reads that could be merged were kept, as well as reads\n",
    "with both primers, reads without Ns, and reads longer than 32 nucleotide\n",
    "after primer trimming. In part 2, clusters with chimeric seed, clusters\n",
    "with a low quality seed, and clusters with a low abundance and seen in\n",
    "only one or two samples were discarded?\n",
    "\n",
    "It is possible to go further, and to discard clusters containing short\n",
    "(or too long) sequences, or clusters with a similarity to references\n",
    "below a certain threshold, or it is possible to use tools such as\n",
    "`lulu`.\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Metabarcoding’s main challenge is noise. Bioinformatics tools can solve\n",
    "part of the problem, but robust experimental designs with replicates and\n",
    "controls are your safest bet."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
