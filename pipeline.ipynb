{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "========\n",
    "\n",
    "open questions:\n",
    "\n",
    "-   wget fastq files from GitHub,\n",
    "-   all code blocks are launched from \\$HOME and not from the latest\n",
    "    folder?\n",
    "-   update: `extract_expected_error_values()` can be simplified with\n",
    "    vsearch 2.23\n",
    "\n",
    "aim\n",
    "---\n",
    "\n",
    "A fast and accurate pipeline.\n",
    "\n",
    "Contrary to traditional pipelines, most filtering steps are done\n",
    "post-clustering, where the risk of eliminating real molecular diversity\n",
    "is minimized.\n",
    "\n",
    "Please feel free to ask questions at anytime.\n",
    "\n",
    "Google Colab\n",
    "------------\n",
    "\n",
    "Let’s explore the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "date\n",
    "whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are `root`! Maximal clearance level, we can do anything we want.\n",
    "\n",
    "Check OS version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "lsb_release -a\n",
    "uname -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operating system is Ubuntu LTS 20.04. LTS stands for long-term\n",
    "support, the most recent LTS at the time of writing is 22.04. Version\n",
    "20.04 will be supported until April 2025.\n",
    "\n",
    "Check basic utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "bash --version\n",
    "git --version\n",
    "gcc --version\n",
    "python --version\n",
    "R --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R, Git and the compilation toolchain are already installed.\n",
    "\n",
    "The `gcc` version is a bit old (9.2), and might not allow to compile\n",
    "code based on very recent standards (e.g.; `mumu` wich uses C++20\n",
    "features).\n",
    "\n",
    "What about hardware resources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "df -h\n",
    "cat /proc/cpuinfo\n",
    "cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Google colab instances are virtual x86-64 machines with 2\n",
    "CPU-cores (Intel Xeon @ 2.20GHz), 12 GB of RAM and 85 GB of storage\n",
    "space.\n",
    "\n",
    "I will assume that you also have [python](https://www.python.org/)\n",
    "(version 3.7 or more), [R](https://cran.r-project.org/) (version 4.0 or\n",
    "more), and [bash](https://www.gnu.org/software/bash/) (version 4 or\n",
    "more).\n",
    "\n",
    "Is it possible to pass data from one `shell` code block to another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "pwd\n",
    "mkdir -p tmp\n",
    "cd ./tmp/0\n",
    "pwd\n",
    "i=5\n",
    "export j=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "pwd\n",
    "echo \"i=\"$i\n",
    "echo \"j=\"$j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that in `%%shell` code blocks, `cd` moves, variable\n",
    "declarations, and function declarations are limited to the current block\n",
    "(no effect on downstream code blocks).\n",
    "\n",
    "install dependencies\n",
    "--------------------\n",
    "\n",
    "Let’s create some folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "mkdir -p src data references results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to install [vsearch](https://github.com/torognes/vsearch),\n",
    "[cutadapt](https://github.com/marcelm/cutadapt/), and\n",
    "[swarm](https://github.com/torognes/swarm).\n",
    "\n",
    "Installing [lulu](https://github.com/tobiasgf/lulu) or\n",
    "[mumu](https://github.com/frederic-mahe/mumu) is not necessary in this\n",
    "particular pipeline.\n",
    "\n",
    "### install cutadapt\n",
    "\n",
    "We could install the Ubuntu version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "apt update\n",
    "apt search cutadapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it is a bit old. Let’s install `cutadapt` with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install --upgrade cutadapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cutadapt --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install swarm\n",
    "\n",
    "We could install `swarm` and `vsearch` using `conda`, but for\n",
    "educational purposes, let’s compile them ourselves. We will put their\n",
    "source code in the `/tmp` folder (cleaned during each reboot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd /tmp/\n",
    "git clone https://github.com/torognes/swarm.git\n",
    "cd ./swarm/\n",
    "make --jobs\n",
    "make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "swarm --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "rm --recursive /tmp/swarm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install vsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd /tmp/\n",
    "git clone https://github.com/torognes/vsearch.git\n",
    "cd ./vsearch/\n",
    "./autogen.sh\n",
    "./configure CFLAGS=\"-O3\" CXXFLAGS=\"-O3\"\n",
    "make --jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "vsearch --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "rm --recursive /tmp/vsearch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install python scripts\n",
    "\n",
    "In the second part of the pipeline, we are going to use python scripts\n",
    "to build and update occurrence tables, and to compute last-common\n",
    "ancestor taxonomic assignments.\n",
    "\n",
    "The scripts are available on GitHub, let’s download the latest versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./src/\n",
    "\n",
    "## occurrence table creation\n",
    "URL=\"https://raw.githubusercontent.com/frederic-mahe/fred-metabarcoding-pipeline/master/src\"\n",
    "wget \"${URL}/OTU_cleaver.py\"\n",
    "wget \"${URL}/OTU_contingency_table_filtered.py\"\n",
    "wget \"${URL}/OTU_table_updater.py\"\n",
    "\n",
    "## taxonomic assignment\n",
    "URL=\"https://raw.githubusercontent.com/frederic-mahe/stampa/master\"\n",
    "wget \"${URL}/stampa_merge.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequencing data\n",
    "---------------\n",
    "\n",
    "A subset of the Neotropical Forest Soil dataset\n",
    "([PRJNA317860](https://www.ebi.ac.uk/ena/browser/view/PRJNA317860);\n",
    "[Mahé et al., 2017](https://doi.org/10.1038/s41559-017-0091)),\n",
    "corresponding to the following ENA/SRA run accessions:\n",
    "\n",
    "| runs        | sample |\n",
    "|-------------|--------|\n",
    "| SRR23272700 | B070   |\n",
    "| SRR23272716 | B030   |\n",
    "| SRR23272737 | B100   |\n",
    "| SRR23272741 | B060   |\n",
    "| SRR23272752 | B050   |\n",
    "| SRR23272767 | L040   |\n",
    "| SRR23272778 | L030   |\n",
    "| SRR23272788 | B090   |\n",
    "| SRR23272799 | B080   |\n",
    "| SRR23272803 | B040   |\n",
    "| SRR23272822 | L080   |\n",
    "| SRR23272833 | L070   |\n",
    "| SRR23272848 | L020   |\n",
    "| SRR23272859 | L010   |\n",
    "| SRR23272860 | B020   |\n",
    "| SRR23272861 | B010   |\n",
    "| SRR23272874 | L090   |\n",
    "| SRR23272881 | L100   |\n",
    "| SRR23272890 | L060   |\n",
    "| SRR23272901 | L050   |\n",
    "\n",
    "and subsampled at 1%, using `vsearch`:\n",
    "\n",
    "``` shell\n",
    "function subsample() {\n",
    "    local -ri SEED=1\n",
    "    local -r PERCENTAGE=\"1.0\"\n",
    "    local -r SUBSAMPLED_FASTQ=\"$(sed 's/NG-7070_// ; s/_lib.*_1976//' <<< ${1})\"\n",
    "\n",
    "    vsearch \\\n",
    "        --fastx_subsample \"${1}\" \\\n",
    "        --randseed \"${SEED}\" \\\n",
    "        --sample_pct \"${PERCENTAGE}\" \\\n",
    "        --quiet \\\n",
    "        --fastqout - | \\\n",
    "        gzip - > \"${SUBSAMPLED_FASTQ}\"\n",
    "}\n",
    "\n",
    "\n",
    "export -f subsample\n",
    "\n",
    "find . -name \"NG-7070_*.fastq.gz\" -type f -exec bash -c 'subsample \"$0\"' {} \\;\n",
    "```\n",
    "\n",
    "This is a very powerful and robust way to find and subsample all fastq\n",
    "files (it will find every file, including files in sub-folders). You\n",
    "might be more familiar with a loop-based approach:\n",
    "\n",
    "``` shell\n",
    "for FASTQ in \"NG-7070_*.fastq.gz\" ; do\n",
    "    subsample \"${FASTQ}\"\n",
    "done\n",
    "```\n",
    "\n",
    "Note: in a pair of R1 and R2 fastq files, both files have the same\n",
    "number of reads, so using a fix seed (not zero) guarantees that\n",
    "subsamplings results for both R1 and R2 fastq files are identical (same\n",
    "number of reads, same reads, in the same order)\n",
    "\n",
    "These 20 runs represent 4 GB of compressed data. To save time and\n",
    "energy, we are going to download the subsampled files directly (roughly\n",
    "40 MB in total). The files are hosted on my GitHub account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "\n",
    "URL=\"https://github.com/frederic-mahe/BIO9905MERG1_vsearch_swarm_pipeline/raw/main/data\"\n",
    "\n",
    "wget \"${URL}/MD5SUM\"\n",
    "for SAMPLE in B010 B020 B030 B040 B050 B060 B070 B080 B090 B100 L010 L020 L030 L040 L050 L060 L070 L080 L090 L100 ; do\n",
    "    for READ in 1 2 ; do\n",
    "        wget \"${URL}/${SAMPLE}_1_${READ}.fastq.gz\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "md5sum -c MD5SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial situation: fastq files are already demultiplexed, we have a pair\n",
    "of R1 and R2 files for each sample.\n",
    "\n",
    "Part 1: from fastq files to fasta files\n",
    "---------------------------------------\n",
    "\n",
    "The pipeline is divided into two parts. A first part where each sample\n",
    "is processed individually. And a second part where samples are pooled to\n",
    "produce an occurrence table.\n",
    "\n",
    "In this first part of the pipeline, we will: 1. merge R1 and R2, 1. trim\n",
    "primers, 1. convert fastq to fasta, 1. extract expected errors (for a\n",
    "later *quality-based filtering*), 1. dereplicate fasta, 1. per-sample\n",
    "clustering (for a later *cluster cleaving*)\n",
    "\n",
    "The code below uses named pipes (`fifo`) to avoid writing intermediate\n",
    "results to mass storage. The goal is to speed up processing, and to make\n",
    "the code more modular and clearer. On the other hand, `fifo`s are tricky\n",
    "to use, as you must remember to launch producers and consumers in the\n",
    "backgroup before running the last consumer.\n",
    "\n",
    "Don’t panic! While the script is running, we are going to look at each\n",
    "function and explain what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./data/\n",
    "export LC_ALL=C\n",
    "\n",
    "## ------------------------------------------------------------ define variables\n",
    "declare -r PRIMER_F=\"CCAGCASCYGCGGTAATTCC\"\n",
    "declare -r PRIMER_R=\"ACTTTCGTTCTTGATYRA\"\n",
    "declare -r FASTQ_NAME_PATTERN=\"*_1.fastq.gz\"\n",
    "declare -ri THREADS=2\n",
    "declare -r CUTADAPT_OPTIONS=\"--minimum-length 32 --cores=${THREADS} --discard-untrimmed\"\n",
    "declare -r CUTADAPT=\"$(which cutadapt) ${CUTADAPT_OPTIONS}\"  # cutadapt 4.1 or more recent\n",
    "declare -r SWARM=\"$(which swarm)\"  # swarm 3.0 or more recent\n",
    "declare -r VSEARCH=\"$(which vsearch) --quiet\"  # vsearch 2.21.1 or more recent\n",
    "declare -ri ENCODING=33\n",
    "declare -r MIN_F=$(( ${#PRIMER_F} * 2 / 3 ))  # match is >= 2/3 of primer length\n",
    "declare -r MIN_R=$(( ${#PRIMER_R} * 2 / 3 ))\n",
    "declare -r FIFOS=$(echo fifo_{merged,trimmed}_fastq fifo_filtered_fasta{,_bis})\n",
    "declare -i TICKER=0\n",
    "\n",
    "## ------------------------------------------------------------------- functions\n",
    "revcomp() {\n",
    "    # reverse-complement a DNA/RNA IUPAC string\n",
    "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
    "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
    "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
    "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
    "}\n",
    "\n",
    "merge_fastq_pair() {\n",
    "    ${VSEARCH} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --fastq_mergepairs \"${FORWARD}\" \\\n",
    "        --reverse \"${REVERSE}\" \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --fastq_allowmergestagger \\\n",
    "        --fastqout fifo_merged_fastq 2> \"${SAMPLE}.log\" &\n",
    "}\n",
    "\n",
    "trim_primers() {\n",
    "    # search forward primer in both normal and revcomp: now all reads\n",
    "    # are in the same orientation\n",
    "    ${CUTADAPT} \\\n",
    "        --revcomp \\\n",
    "        --front \"${PRIMER_F};rightmost\" \\\n",
    "        --overlap \"${MIN_F}\" fifo_merged_fastq 2>> \"${SAMPLE}.log\" | \\\n",
    "        ${CUTADAPT} \\\n",
    "            --adapter \"${ANTI_PRIMER_R}\" \\\n",
    "            --overlap \"${MIN_R}\" \\\n",
    "            --max-n 0 - > fifo_trimmed_fastq 2>> \"${SAMPLE}.log\" &\n",
    "}\n",
    "\n",
    "convert_fastq_to_fasta() {\n",
    "    # use SHA1 values as sequence names,\n",
    "    # compute expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --fastq_filter fifo_trimmed_fastq \\\n",
    "        --relabel_sha1 \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --eeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --fastaout - 2>> \"${SAMPLE}.log\" | \\\n",
    "        tee fifo_filtered_fasta_bis > fifo_filtered_fasta &\n",
    "}\n",
    "\n",
    "extract_expected_error_values() {\n",
    "    # extract ee for future quality filtering (keep the lowest\n",
    "    # observed expected error value for each unique sequence)\n",
    "    local -ri length_of_sequence_IDs=40\n",
    "    paste - - < fifo_filtered_fasta_bis | \\\n",
    "        awk 'BEGIN {FS = \"[>;=\\t]\"} {print $2, $4, length($NF)}' | \\\n",
    "        sort --key=3,3n --key=1,1d --key=2,2n | \\\n",
    "        uniq --check-chars=${length_of_sequence_IDs} > \"${SAMPLE}.qual\" &\n",
    "}\n",
    "\n",
    "dereplicate_fasta() {\n",
    "    # dereplicate and discard expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --derep_fulllength fifo_filtered_fasta \\\n",
    "        --sizeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --xee \\\n",
    "        --output \"${SAMPLE}.fas\" 2>> \"${SAMPLE}.log\"\n",
    "}\n",
    "\n",
    "list_local_clusters() {\n",
    "    # retain only clusters with more than 2 reads\n",
    "    # (do not use the fastidious option here)\n",
    "    ${SWARM} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --differences 1 \\\n",
    "        --usearch-abundance \\\n",
    "        --log /dev/null \\\n",
    "        --output-file /dev/null \\\n",
    "        --statistics-file - \\\n",
    "        \"${SAMPLE}.fas\" | \\\n",
    "        awk 'BEGIN {FS = OFS = \"\\t\"} $2 > 2' > \"${SAMPLE}.stats\"\n",
    "}\n",
    "\n",
    "## ------------------------------------------------------------------------ main\n",
    "# from raw fastq files to ready-to-use sample files\n",
    "declare -r ANTI_PRIMER_R=\"$(revcomp \"${PRIMER_R}\")\"\n",
    "find . -name \"${FASTQ_NAME_PATTERN}\" -type f -print0 | \\\n",
    "    while IFS= read -r -d '' FORWARD ; do\n",
    "        TICKER=$(( $TICKER + 1 ))\n",
    "        echo -e \"${TICKER}\\t${FORWARD}\"\n",
    "        REVERSE=\"${FORWARD/_1\\./_2.}\"  # adapt to fastq name patterns\n",
    "        SAMPLE=\"${FORWARD/_1_1.*/}\"\n",
    "\n",
    "        # clean (remove older files, if any)\n",
    "        rm --force \"${SAMPLE}\".{fas,qual,log,stats} ${FIFOS}\n",
    "        mkfifo ${FIFOS}\n",
    "\n",
    "        merge_fastq_pair\n",
    "        trim_primers\n",
    "        convert_fastq_to_fasta\n",
    "        extract_expected_error_values\n",
    "        dereplicate_fasta\n",
    "        list_local_clusters\n",
    "\n",
    "        # clean (make sure fifos are not reused)\n",
    "        rm ${FIFOS}\n",
    "        unset FORWARD REVERSE SAMPLE\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adapt this code to another dataset, you just need to change the\n",
    "primer sequences in the initial block of variables, and the raw fastq\n",
    "file search pattern and sample file naming if your raw fastq files\n",
    "follow another naming rule (in the final `while` loop).\n",
    "\n",
    "### variable declarations\n",
    "\n",
    "``` shell\n",
    "VAR1=\"some text\"\n",
    "declare -r VAR1=\"more text\"\n",
    "declare -ri VAR2=42\n",
    "```\n",
    "\n",
    "Here, we are using `declare -r` to indicate that `VAR1` is a constant.\n",
    "If we try to modify it somewhere in the code, that’s a bug, and the\n",
    "execution will stop with an error message.\n",
    "\n",
    "`declare -ri` indicates that `VAR2` is a constant integer.\n",
    "\n",
    "### R1 and R2 merging\n",
    "\n",
    "``` shell\n",
    "merge_fastq_pair() {\n",
    "    ${VSEARCH} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --fastq_mergepairs \"${FORWARD}\" \\\n",
    "        --reverse \"${REVERSE}\" \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --fastq_allowmergestagger \\\n",
    "        --fastqout fifo_merged_fastq 2> \"${SAMPLE}.log\" &\n",
    "}\n",
    "```\n",
    "\n",
    "before merging:\n",
    "\n",
    "    R1: ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequen\n",
    "                                                   ||||\n",
    "                                                   quence-ESREVER_REMIRP-GAT-ROTPADA :R2\n",
    "\n",
    "after merging:\n",
    "\n",
    "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "A minimal overlap and similarity is required for the merging. Reads that\n",
    "can’t be merged are discarded. `vsearch` re-computes the quality values\n",
    "of the overlaping positions (double observations, the error probability\n",
    "must be re-evaluated).\n",
    "\n",
    "### reverse-complement\n",
    "\n",
    "``` shell\n",
    "revcomp() {\n",
    "    # reverse-complement a DNA/RNA IUPAC string\n",
    "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
    "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
    "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
    "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
    "}\n",
    "```\n",
    "\n",
    "That function takes a primer sequence, for example `ACTTTCGTTCTTGATYRA`\n",
    "and outputs the reverse-complement of that sequence\n",
    "(`TYRATCAAGAACGAAAGT`). This is useful when searching for primers after\n",
    "merging.\n",
    "\n",
    "After merging, the reverse primer is reverse-complemented.\n",
    "\n",
    "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "### trim primers\n",
    "\n",
    "After merging, we use cutadapt to trim primers:\n",
    "\n",
    "``` shell\n",
    "trim_primers() {\n",
    "    # search forward primer in both normal and revcomp: now all reads\n",
    "    # are in the same orientation\n",
    "    ${CUTADAPT} \\\n",
    "        --revcomp \\\n",
    "        --front \"${PRIMER_F};rightmost\" \\\n",
    "        --overlap \"${MIN_F}\" fifo_merged_fastq 2>> \"${SAMPLE}.log\" | \\\n",
    "        ${CUTADAPT} \\\n",
    "            --adapter \"${ANTI_PRIMER_R}\" \\\n",
    "            --overlap \"${MIN_R}\" \\\n",
    "            --max-n 0 - > fifo_trimmed_fastq 2>> \"${SAMPLE}.log\" &\n",
    "}\n",
    "```\n",
    "\n",
    "Before:\n",
    "\n",
    "    ADAPTOR-TAG-PRIMER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "the first call removes the *rightmost* forward primer (and everything\n",
    "before):\n",
    "\n",
    "    actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "\n",
    "the second call removes the reverse primer (and everything after).\n",
    "\n",
    "    actual_target_sequence\n",
    "\n",
    "At either step, if a primer is not found, the read is discarded.\n",
    "\n",
    "`--revcomp`: some sequencing protocols can produce reads in random\n",
    "orientations (R1 contains a mix of forward and reverse reads). That\n",
    "option has the effect of re-orienting the reads.\n",
    "\n",
    "`--overlap`: by default, cutadapt considers that an overlap of three\n",
    "nucleotides is enough for a match. Here we require a minimal overlap\n",
    "equal to 2/3rd of the length of our primers.\n",
    "\n",
    "    Match:\n",
    "\n",
    "       MER_FORWARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "       |||||||||||\n",
    "    PRIMER_FORWARD\n",
    "\n",
    "\n",
    "    No Match\n",
    "              WARD-actual_target_sequence-ESREVER_REMIRP-GAT-ROTPADA\n",
    "              ||||\n",
    "    PRIMER_FORWARD\n",
    "\n",
    "Note: `--max-n 0` reads with uncertain nucleotides (`N`) are discarded.\n",
    "\n",
    "### convert fastq to fasta\n",
    "\n",
    "A simple format conversion:\n",
    "\n",
    "``` shell\n",
    "convert_fastq_to_fasta() {\n",
    "    # use SHA1 values as sequence names,\n",
    "    # compute expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --fastq_filter fifo_trimmed_fastq \\\n",
    "        --relabel_sha1 \\\n",
    "        --fastq_ascii \"${ENCODING}\" \\\n",
    "        --eeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --fastaout - 2>> \"${SAMPLE}.log\" | \\\n",
    "        tee fifo_filtered_fasta_bis > fifo_filtered_fasta &\n",
    "}\n",
    "```\n",
    "\n",
    "For example, a fastq entry such as:\n",
    "\n",
    "    @s1\n",
    "    AAAA\n",
    "    +\n",
    "    IIII\n",
    "\n",
    "will be transformed into a fasta entry, with a sequence-derived name\n",
    "(hash value of the sequence), and the computed expected error (later\n",
    "used for quality filtering):\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
    "    AAAA\n",
    "\n",
    "### extract expected error values\n",
    "\n",
    "Extract and store read lengths and expected error values (later used for\n",
    "quality filtering). If two reads have the exact same sequence, keep the\n",
    "best (lowest) expected error.\n",
    "\n",
    "``` shell\n",
    "extract_expected_error_values() {\n",
    "    # extract ee for future quality filtering (keep the lowest\n",
    "    # observed expected error value for each unique sequence)\n",
    "    local -ri length_of_sequence_IDs=40\n",
    "    paste - - < fifo_filtered_fasta_bis | \\\n",
    "        awk 'BEGIN {FS = \"[>;=\\t]\"} {print $2, $4, length($NF)}' | \\\n",
    "        sort --key=3,3n --key=1,1d --key=2,2n | \\\n",
    "        uniq --check-chars=${length_of_sequence_IDs} > \"${SAMPLE}.qual\" &\n",
    "}\n",
    "```\n",
    "\n",
    "That function could be simplified now that `vsearch` 2.23 can add length\n",
    "attributes `;length=123` to fastq and fasta headers.\n",
    "\n",
    "### dereplicate\n",
    "\n",
    "The first significant lossless reduction of our dataset! In\n",
    "metabarcoding datasets, some sequences are rare and only observed once,\n",
    "whereas some sequences are present in many exact copies. When coded\n",
    "correctly, finding identical sequences is a very fast and efficient\n",
    "operation:\n",
    "\n",
    "``` shell\n",
    "dereplicate_fasta() {\n",
    "    # dereplicate and discard expected error values (ee)\n",
    "    ${VSEARCH} \\\n",
    "        --derep_fulllength fifo_filtered_fasta \\\n",
    "        --sizeout \\\n",
    "        --fasta_width 0 \\\n",
    "        --xee \\\n",
    "        --output \"${SAMPLE}.fas\" 2>> \"${SAMPLE}.log\"\n",
    "}\n",
    "```\n",
    "\n",
    "For instance, this fasta file:\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
    "    AAAA\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;ee=0.0004000\n",
    "    AAAA\n",
    "\n",
    "will be dereplicated into this one:\n",
    "\n",
    "    >e2512172abf8cc9f67fdd49eb6cacf2df71bbad3;size=2\n",
    "    AAAA\n",
    "\n",
    "Note that expected error values are removed by the `-xee` option, and\n",
    "that a new attribute `;size=2` has been added to represent the fact that\n",
    "this particular sequence has been observed twice.\n",
    "\n",
    "### list local clusters\n",
    "\n",
    "Later in this analysis, we will need to search for cluster co-occurences\n",
    "on a per-sample basis. We quickly generate clusters and a list of\n",
    "cluster seeds with `swarm` and store the results (reminder: everything\n",
    "is done at the sample level in that first part of the pipeline). Here we\n",
    "use `swarm` for the first time, I will give more details when `swarm`\n",
    "will be used to process the whole dataset (pooled samples), in the\n",
    "second part of the pipeline.\n",
    "\n",
    "``` shell\n",
    "list_local_clusters() {\n",
    "    # retain only clusters with more than 2 reads\n",
    "    # (do not use the fastidious option here)\n",
    "    ${SWARM} \\\n",
    "        --threads \"${THREADS}\" \\\n",
    "        --differences 1 \\\n",
    "        --usearch-abundance \\\n",
    "        --log /dev/null \\\n",
    "        --output-file /dev/null \\\n",
    "        --statistics-file - \\\n",
    "        \"${SAMPLE}.fas\" | \\\n",
    "        awk 'BEGIN {FS = OFS = \"\\t\"} $2 > 2' > \"${SAMPLE}.stats\"\n",
    "}\n",
    "```\n",
    "\n",
    "Here, `swarm` reads a fasta file, representing a sample, and produces a\n",
    "table looking like that:\n",
    "\n",
    "| uniq | abundance | seed                                     | abundance | singletons | layers | steps |\n",
    "|------|-----------|------------------------------------------|-----------|------------|--------|-------|\n",
    "| 5897 | 22010     | 3db68d2f77252793f23d7089d0d4103eb8942dcb | 3612      | 4600       | 8      | 8     |\n",
    "| 5528 | 15548     | 45211af6b7811bf45b5d3694054b800b5b13efd4 | 3728      | 4488       | 9      | 9     |\n",
    "| 3058 | 13452     | 47e639615ad19c8dededae45f38beb52c4e9861d | 3542      | 2252       | 9      | 9     |\n",
    "| 2172 | 8268      | 307ab3d7f513adc74854dd72e817fe02f7601db2 | 3722      | 1639       | 8      | 8     |\n",
    "| 1453 | 6754      | e52a67c43758a5f9a39b4cb42ac10cf41958e1d4 | 3563      | 1053       | 4      | 4     |\n",
    "| 1437 | 8836      | 962e2976f63ec09db8361ce4a498f50ade4b1674 | 3712      | 1046       | 4      | 4     |\n",
    "| 1303 | 6307      | 79ae1f05f177ad948d18dc86a7944772c63273b9 | 3559      | 971        | 6      | 6     |\n",
    "| 1126 | 6032      | b0835c4fba3d39e25059371902b0774741e3a57d | 3562      | 748        | 5      | 5     |\n",
    "| 759  | 5355      | 91be8585fcad245e9f3a53578207c58355426583 | 3603      | 540        | 4      | 4     |\n",
    "| 743  | 5981      | 42871cf2b4deaa78cbd1c163847567685c56d44e | 3654      | 567        | 4      | 4     |\n",
    "\n",
    "In the second part of the pipeline, we are going to use that table as a\n",
    "list of sequences that played the role of cluster seeds in this\n",
    "particular sample. Don’t worry, that part will be explained in details.\n",
    "\n",
    "### loop over each pair of fastq files\n",
    "\n",
    "Finally, we search for all fastq R1 files (`find`) and we apply our\n",
    "functions. If we remove the clutter, it looks like this:\n",
    "\n",
    "``` shell\n",
    "find . -name \"${FASTQ_NAME_PATTERN}\" -type f -print0 | \\\n",
    "    while IFS= read -r -d '' FORWARD ; do\n",
    "        ...\n",
    "        merge_fastq_pair\n",
    "        trim_primers\n",
    "        convert_fastq_to_fasta\n",
    "        extract_expected_error_values\n",
    "        dereplicate_fasta\n",
    "        list_local_clusters\n",
    "        ...\n",
    "    done\n",
    "```\n",
    "\n",
    "That’s it for the first part of the pipeline!\n",
    "\n",
    "taxonomic references\n",
    "--------------------\n",
    "\n",
    "Before tackling the second part of the pipeline, we need to prepare our\n",
    "reference database for the taxonomic assignment of our environmental\n",
    "sequences.\n",
    "\n",
    "We are working with 18S V4 amplicons. We could use Silva SSU, but\n",
    "[PR2](https://github.com/pr2database/pr2database), the Protist Ribosomal\n",
    "Reference database, is a well-curated and eukaryote specific database of\n",
    "SSU (18S) references. Let’s use the latest version (5.0), published\n",
    "earlier this month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./references/\n",
    "\n",
    "revcomp() {\n",
    "    # reverse-complement a DNA/RNA IUPAC string\n",
    "    [[ -z \"${1}\" ]] && { echo \"error: empty string\" ; exit 1 ; }\n",
    "    local -r nucleotides=\"acgturykmbdhvswACGTURYKMBDHVSW\"\n",
    "    local -r complements=\"tgcaayrmkvhdbswTGCAAYRMKVHDBSW\"\n",
    "    tr \"${nucleotides}\" \"${complements}\" <<< \"${1}\" | rev\n",
    "}\n",
    "\n",
    "## download PR2 (UTAX version)\n",
    "declare -r URL=\"https://github.com/pr2database/pr2database/releases/download\"\n",
    "declare -r VERSION=\"5.0.0\"\n",
    "declare -r SOURCE=\"pr2_version_${VERSION}_SSU_UTAX.fasta\"\n",
    "[[ -e \"${SOURCE}.gz\" ]] || wget \"${URL}/v${VERSION}/${SOURCE}.gz\"\n",
    "\n",
    "\n",
    "## extract the V4 region (primers from Stoeck et al. 2010)\n",
    "declare -r PRIMER_F=\"CCAGCASCYGCGGTAATTCC\"\n",
    "declare -r PRIMER_R=\"ACTTTCGTTCTTGATYRA\"\n",
    "declare -r ANTI_PRIMER_R=\"$(revcomp \"${PRIMER_R}\")\"\n",
    "declare -r OUTPUT=\"${SOURCE/_UTAX*/}_${PRIMER_F}_${PRIMER_R}.fas\"\n",
    "declare -r LOG=\"${OUTPUT/.fas/.log}\"\n",
    "declare -r MIN_LENGTH=\"32\"\n",
    "declare -r ERROR_RATE=\"0.2\"\n",
    "declare -r MIN_F=$(( ${#PRIMER_F} * 1 / 3 ))\n",
    "declare -r MIN_R=$(( ${#PRIMER_R} * 1 / 3 ))\n",
    "declare -r OPTIONS=\"--minimum-length ${MIN_LENGTH} --discard-untrimmed --error-rate ${ERROR_RATE}\"\n",
    "declare -r CUTADAPT=\"$(which cutadapt) ${OPTIONS}\"\n",
    "\n",
    "zcat \"${SOURCE}.gz\" | \\\n",
    "    dos2unix | \\\n",
    "    sed '/^>MF423350/ s/s:Heterocapsa steinii/s:Heterocapsa steinii/\n",
    "         /^>/ s/;tax=k:/ /\n",
    "         /^>/ s/,[dpcofgs]:/|/g\n",
    "         /^>/ ! s/U/T/g' | \\\n",
    "     ${CUTADAPT} \\\n",
    "         --revcomp \\\n",
    "         --front \"${PRIMER_F}\" \\\n",
    "         --overlap \"${MIN_F}\" - 2> \"${LOG}\" | \\\n",
    "         ${CUTADAPT} \\\n",
    "         --adapter \"${ANTI_PRIMER_R}\" \\\n",
    "         --overlap \"${MIN_R}\" - > \"${OUTPUT}\" 2>> \"${LOG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won’t go into details here, but you might notice some familiar code\n",
    "such as `revcomp()` and two calls to `cutadapt`. The goal here is to\n",
    "download and trim the reference sequences:\n",
    "\n",
    "    reference 1: xxxxxxxx-PRIMER_F-target_region-PRIMER_R-xxxxx\n",
    "    reference 2:            IMER_F-target_region-PRIMER_R-xxxxxxxxxxx\n",
    "    reference 3:                    arget-region-PRIMER_R-xxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "Discard references that do not contain our target region (flanked by our\n",
    "primers), and trim the primers. The reference dataset is now much\n",
    "smaller, and limited to our target region:\n",
    "\n",
    "    reference 1:                   target_region\n",
    "    reference 2:                   target_region\n",
    "\n",
    "Note: while preparing this, I’ve found a small bug in the latest PR2\n",
    "release ([weird character in one species\n",
    "name](https://github.com/pr2database/pr2database/issues/37))\n",
    "\n",
    "Part 2: from fasta files to an annotated occurrence table\n",
    "---------------------------------------------------------\n",
    "\n",
    "-   install python scripts first,\n",
    "-   fix taxonomic assignment,\n",
    "-   remove unused variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cd ./results/\n",
    "export LC_ALL=C\n",
    "\n",
    "## ------------------------------------------------------------ define variables\n",
    "declare -r PROJECT=\"Neotropical_soils_18S_V4\"\n",
    "declare -r DATA_FOLDER=\"../data/\"\n",
    "declare -r SWARM=\"$(which swarm)\"  # swarm 3.0 or more recent\n",
    "declare -r VSEARCH=\"$(which vsearch)\"  # vsearch 2.21 or more recent\n",
    "declare -ri THREADS=2\n",
    "declare -ri RESOLUTION=1\n",
    "declare -ri FILTER=2\n",
    "declare -r SRC=\"../src\"\n",
    "declare -r OTU_CLEAVER=\"OTU_cleaver.py\"\n",
    "declare -r OTU_TABLE_BUILDER=\"OTU_contingency_table_filtered.py\"\n",
    "declare -r OTU_TABLE_UPDATER=\"OTU_table_updater.py\"\n",
    "declare -r STAMPA_MERGE=\"stampa_merge.py\"\n",
    "declare -r DATABASE=\"../references/pr2_version_5.0.0_SSU_CCAGCASCYGCGGTAATTCC_ACTTTCGTTCTTGATYRA.fas\"\n",
    "\n",
    "## ---------------------------------------------------------- global clustering\n",
    "echo \"run global clustering and chimera detection...\"\n",
    "\n",
    "## variables and file names\n",
    "N_SAMPLES=$(find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "                -type f ! -empty -print0 | tr -d -c '\\0' | wc -m)\n",
    "FINAL_FASTA=\"${PROJECT}_${N_SAMPLES}_samples.fas\"\n",
    "QUALITY_FILE=\"${FINAL_FASTA%.*}.qual\"\n",
    "DISTRIBUTION_FILE=\"${FINAL_FASTA%.*}.distr\"\n",
    "POTENTIAL_SUB_SEEDS=\"${FINAL_FASTA%.*}_per_sample_OTUs.stats\"\n",
    "LOG=\"${FINAL_FASTA%.*}.log\"\n",
    "OUTPUT_SWARMS=\"${FINAL_FASTA%.*}_${RESOLUTION}f.swarms\"\n",
    "OUTPUT_LOG=\"${FINAL_FASTA%.*}_${RESOLUTION}f.log\"\n",
    "OUTPUT_STATS=\"${FINAL_FASTA%.*}_${RESOLUTION}f.stats\"\n",
    "OUTPUT_STRUCT=\"${FINAL_FASTA%.*}_${RESOLUTION}f.struct\"\n",
    "OUTPUT_REPRESENTATIVES=\"${FINAL_FASTA%.*}_${RESOLUTION}f_representatives.fas\"\n",
    "TAXONOMIC_ASSIGNMENTS=\"${OUTPUT_REPRESENTATIVES%.*}.results\"\n",
    "UCHIME_RESULTS=\"${OUTPUT_REPRESENTATIVES%.*}.uchime\"\n",
    "UCHIME_LOG=\"${OUTPUT_REPRESENTATIVES%.*}.log\"\n",
    "OTU_TABLE=\"${FINAL_FASTA%.*}.OTU.filtered.cleaved.table\"\n",
    "OUTPUT_TABLE=\"${OTU_TABLE%.*}.nosubstringOTUs.table\"\n",
    "\n",
    "## Build expected error file\n",
    "find \"${DATA_FOLDER}\" -name \"*.qual\" \\\n",
    "    -type f ! -empty -print0 | \\\n",
    "    sort -k3,3n -k1,1d -k2,2n --merge --files0-from=- | \\\n",
    "    uniq --check-chars=40 > \"${QUALITY_FILE}\" &\n",
    "\n",
    "\n",
    "## Build distribution file (sequence <-> sample relations)\n",
    "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "    -type f ! -empty -execdir grep -H \"^>\" '{}' \\; | \\\n",
    "    sed 's/.*\\/// ; s/\\.fas:>/\\t/ ; s/;size=/\\t/ ; s/;$//' | \\\n",
    "    awk 'BEGIN {FS = OFS = \"\\t\"} {print $2, $1, $3}' > \"${DISTRIBUTION_FILE}\" &\n",
    "\n",
    "\n",
    "## list all cluster seeds of size > 2\n",
    "find \"${DATA_FOLDER}\" -name \"*.stats\" \\\n",
    "    -type f ! -empty -execdir grep -H \"\" '{}' \\; | \\\n",
    "    sed 's/^\\.\\/// ; s/\\.stats:/\\t/' > \"${POTENTIAL_SUB_SEEDS}\" &\n",
    "\n",
    "\n",
    "## global dereplication\n",
    "find \"${DATA_FOLDER}\" -name \"*.fas\" \\\n",
    "    -type f ! -empty -execdir cat '{}' + | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --derep_fulllength - \\\n",
    "        --sizein \\\n",
    "        --sizeout \\\n",
    "        --log \"${LOG}\" \\\n",
    "        --fasta_width 0 \\\n",
    "        --output \"${FINAL_FASTA}\"\n",
    "\n",
    "\n",
    "## clustering (swarm 3 or more recent)\n",
    "\"${SWARM}\" \\\n",
    "    --differences \"${RESOLUTION}\" \\\n",
    "    --fastidious \\\n",
    "    --usearch-abundance \\\n",
    "    --threads \"${THREADS}\" \\\n",
    "    --internal-structure \"${OUTPUT_STRUCT}\" \\\n",
    "    --output-file \"${OUTPUT_SWARMS}\" \\\n",
    "    --statistics-file \"${OUTPUT_STATS}\" \\\n",
    "    --seeds \"${OUTPUT_REPRESENTATIVES}\" \\\n",
    "    \"${FINAL_FASTA}\" 2> \"${OUTPUT_LOG}\"\n",
    "\n",
    "\n",
    "## fake taxonomic assignment\n",
    "grep \"^>\" \"${OUTPUT_REPRESENTATIVES}\" | \\\n",
    "    sed -r 's/^>//\n",
    "            s/;size=/\\t/\n",
    "            s/;?$/\\t0.0\\tNA\\tNA/' > \"${TAXONOMIC_ASSIGNMENTS}\"\n",
    "\n",
    "\n",
    "## chimera detection\n",
    "## discard sequences with an abundance lower than FILTER\n",
    "## and search for chimeras\n",
    "\"${VSEARCH}\" \\\n",
    "    --fastx_filter \"${OUTPUT_REPRESENTATIVES}\"  \\\n",
    "    --minsize \"${FILTER}\" \\\n",
    "    --fastaout - | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --uchime_denovo - \\\n",
    "        --uchimeout \"${UCHIME_RESULTS}\" \\\n",
    "        2> \"${UCHIME_LOG}\"\n",
    "\n",
    "\n",
    "## ------------------------------------------------------------------- cleaving\n",
    "echo \"run cleaving...\"\n",
    "\n",
    "## split OTUs\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_CLEAVER}\" \\\n",
    "    --global_stats \"${OUTPUT_STATS}\" \\\n",
    "    --per_sample_stats \"${POTENTIAL_SUB_SEEDS}\" \\\n",
    "    --struct \"${OUTPUT_STRUCT}\" \\\n",
    "    --swarms \"${OUTPUT_SWARMS}\" \\\n",
    "    --fasta \"${FINAL_FASTA}\"\n",
    "\n",
    "## fake taxonomic assignment\n",
    "grep \"^>\" \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
    "    sed -r 's/^>//\n",
    "            s/;size=/\\t/\n",
    "            s/;?$/\\t0.0\\tNA\\tNA/' > \"${OUTPUT_REPRESENTATIVES%.*}.results2\"\n",
    "\n",
    "## chimera detection (only down to the smallest newly cleaved OTU)\n",
    "LOWEST_ABUNDANCE=$(sed -rn \\\n",
    "    '/^>/ s/.*;size=([0-9]+);?/\\1/p' \\\n",
    "    \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
    "    sort -n | \\\n",
    "    head -n 1)\n",
    "\n",
    "# sort and filter by abundance (default to an abundance of 1), search\n",
    "# for chimeras\n",
    "cat \"${OUTPUT_REPRESENTATIVES}\" \"${OUTPUT_REPRESENTATIVES}2\" | \\\n",
    "    \"${VSEARCH}\" \\\n",
    "        --sortbysize - \\\n",
    "        --sizein \\\n",
    "        --minsize ${LOWEST_ABUNDANCE:-1} \\\n",
    "        --sizeout \\\n",
    "        --output - | \\\n",
    "        \"${VSEARCH}\" \\\n",
    "            --uchime_denovo - \\\n",
    "            --uchimeout \"${UCHIME_RESULTS}2\" \\\n",
    "            2> \"${OUTPUT_REPRESENTATIVES%.*}.log2\"\n",
    "\n",
    "unset LOWEST_ABUNDANCE\n",
    "\n",
    "\n",
    "## ------------------------------------------------------------ first OTU table\n",
    "echo \"build first OTU table...\"\n",
    "\n",
    "# build OTU table\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_TABLE_BUILDER}\" \\\n",
    "    --representatives <(cat \"${OUTPUT_REPRESENTATIVES}\"{,2}) \\\n",
    "    --stats <(cat \"${OUTPUT_STATS}\"{,2}) \\\n",
    "    --swarms <(cat \"${OUTPUT_SWARMS}\"{,2}) \\\n",
    "    --chimera <(cat \"${UCHIME_RESULTS}\"{,2}) \\\n",
    "    --quality \"${QUALITY_FILE}\" \\\n",
    "    --assignments <(cat \"${TAXONOMIC_ASSIGNMENTS}\"{2,}) \\\n",
    "    --distribution \"${DISTRIBUTION_FILE}\" > \"${OTU_TABLE}\"\n",
    "\n",
    "\n",
    "# extract fasta sequences from OTU table\n",
    "awk 'NR > 1 {printf \">\"$4\";size=\"$2\";\\n\"$10\"\\n\"}' \"${OTU_TABLE}\" \\\n",
    "    > \"${OTU_TABLE/.table/.fas}\"\n",
    "\n",
    "\n",
    "\n",
    "## ------------------------------------------------------- taxonomic assignment\n",
    "\n",
    "# search for best hits\n",
    "${VSEARCH} \\\n",
    "    --usearch_global \"${OTU_TABLE/.table/.fas}\" \\\n",
    "    --db ${DATABASE} \\\n",
    "    --threads ${THREADS} \\\n",
    "    --dbmask none \\\n",
    "    --qmask none \\\n",
    "    --rowlen 0 \\\n",
    "    --notrunclabels \\\n",
    "    --userfields query+id1+target \\\n",
    "    --maxaccepts 0 \\\n",
    "    --maxrejects 0 \\\n",
    "    --top_hits_only \\\n",
    "    --output_no_hits \\\n",
    "    --id 0.5 \\\n",
    "    --iddef 1 \\\n",
    "    --userout - | \\\n",
    "    sed 's/;size=/_/ ; s/;//' > hits.representatives\n",
    "\n",
    "# in case of multi-best hit, find the last-common ancestor\n",
    "python ${SRC}/${STAMPA_MERGE} $(pwd)\n",
    "\n",
    "# sort by decreasing abundance\n",
    "sort -k2,2nr -k1,1d results.representatives > representatives.results\n",
    "\n",
    "cp representatives.results \"${OTU_TABLE/.table/.results}\"\n",
    "\n",
    "# clean-up\n",
    "# rm results.representatives representatives.results\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------- update OTU table\n",
    "echo \"build final OTU table...\"\n",
    "NEW_TABLE=$(mktemp)\n",
    "\n",
    "python3 \\\n",
    "    \"${SRC}/${OTU_TABLE_UPDATER}\" \\\n",
    "    --old_otu_table \"${OTU_TABLE}\" \\\n",
    "    --new_taxonomy \"${OTU_TABLE/.table/.results}\" \\\n",
    "    --new_otu_table \"${NEW_TABLE}\"\n",
    "\n",
    "# fix OTU sorting\n",
    "(head -n 1 \"${NEW_TABLE}\"\n",
    "    tail -n +2 \"${NEW_TABLE}\" | sort -k2,2nr | nl -n'ln' -w1 | cut --complement -f 2\n",
    ") > \"${OTU_TABLE}2\"\n",
    "\n",
    "# clean up\n",
    "chmod go+r,g-w \"${OTU_TABLE}2\"\n",
    "rm \"${NEW_TABLE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   explain cleaving (as a complement to lulu: separate similar\n",
    "    sequences that do not co-occur)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
